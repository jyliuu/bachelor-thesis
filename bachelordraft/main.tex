\documentclass[11pt, a4paper]{article}
\usepackage[margin=3cm]{geometry} 
\usepackage[english, science, hyperref]{ku-frontpage}
\usepackage[justification=centering]{caption}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,amsthm,bm}
\usepackage{mdframed}
\usepackage{float}
\usepackage[mode=buildnew]{standalone}% requires -shell-escape
\usepackage{mathtools}
\usepackage[
    backend=biber,
    bibstyle=numeric,
    citestyle=authoryear,
    sorting=ynt
]{biblatex}
\addbibresource{references.bib} % This command is used instead of \bibliography{references.bib}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}

\usepackage{subfiles} % Best loaded last in the preamble
\setlength\parindent{0pt}
\DeclareMathOperator*{\argmax}{arg\,max} 
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\indep}{\perp \!\!\! \perp}
\newcommand\norm[1]{\lVert#1\rVert}

\newtheorem{theorem}{Theorem}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\usepackage{mdframed}

\mdfdefinestyle{examplestyle}{%
    linewidth=1pt,
    innerleftmargin=10pt,
    innerrightmargin=10pt,
    skipabove=10pt, % add vertical space above
    skipbelow=10pt, % add vertical space below
    frametitlefont=\bfseries,
    nobreak=true
}
\newtheorem{example}{Example}
\surroundwithmdframed[style=examplestyle]{example}


\setlength\arraycolsep{2 pt}
\setcounter{tocdepth}{2}
\setcounter{secnumdepth}{2}

\newcommand{\q}{q}
\newcommand{\ml}{k}
\newcommand{\btheta}{\theta}
\newcommand{\la}{\psi}
\newcommand{\Sn}{S^n}
\newcommand{\lib}{\Psi}
\newcommand{\lone}{\mathcal{L}}
\newcommand{\empmod}{\mathcal{P}_{\text{emp}}}
\newcommand{\meta}{\phi}
\newcommand{\Meta}{\Phi}
\newcommand{\esl}{\Sigma}
\DeclareMathOperator{\expit}{expit}

\assignment{A Bachelor of Science thesis}
\author{Jinyang Liu}


\title{Super Learners}
\subtitle{and their oracle properties}
\date{Submitted: \today}
\advisor{Supervised by Prof. Thomas Gerds\\ Co-supervised by Prof. Niels Richard Hansen \\Department of Mathematical Sciences \\University of Copenhagen, Denmark}
% \frontpageimage{example.png}

\begin{document}
\begingroup
    \fontencoding{T1}\fontfamily{lmr}\selectfont
    \maketitle
    \tableofcontents
    \newpage
\endgroup


\section{Introduction}
In the context of regression, $ O = (Y, X) $ is an observation for $ Y \in \mathbb{R} $ being the outcome and $ X \in \mathbb{R}^{d} $ the covariate. A natural goal is to estimate a regression function $ \theta $ such that the $ L^2 $-risk or mean squared error $ E(Y - \theta(X))^2 $ is minimized. It turns out that the conditional mean -- called the regression function or regression $ x \mapsto E(Y \mid X = x) $ minimizes the squared error, but consistent estimation of the regression requires us to specify a statistical model $ \mathcal{P} $ for the data-generating distribution $ P \in \mathcal{P} $ for which $ O \sim P $. We typically make certain assumptions about the statistical model, $\mathcal{P}$, in which we believe $P$ resides. For instance, we might assume that $\mathcal{P}$ is a parametric family of distributions, for example a curved exponential family \parencite{lauritzen2023fundamentals}. In doing so we are able to identify the parameters of the distribution $ P $ via maximum liklihood and compute the regression from the estimated parameters.

However, if we are dealing with complex data, there is a risk of misspecifying the model by identifying it as an exponential family. Our assumptions may be wrong. In such situations, it is tempting to utilize non-parametric and data-driven regression methods, such as tree-based algorithms like XGBoost or random forests. Machine learning methods seek to estimate regression function directly, in contrast to parametric statistics where the parameters of the underlying model are estimated first, then a regression function derived as some analytical expression of these parameters and the covariates. The assumptions of these machine learning methods regarding the data-generating distribution are not explicitly specified, but it does not pose a problem for us in achieving our goal. Indeed, it is not necessary for us to identify $ P $ completely if $ P $ can be factored into the conditional distribution $ P_{Y \mid X} $ and the distribution over our covariates $ P_X $, here our goal of estimating the regression function is to estimate $ P_{Y \mid X} $.   

Given an abundance of different ways we can tackle the problem of estimating the regression, we would like to be able to compare the different methods select the best one. In a practical scenario we might have set of different learning algorithms from which we can choose from. Cross-validation can then help determine the risk of each algorithm by splitting the data into training and validation sets, each algorithm would then be fitted on the training set and an empirical risk for each fitted model is then calculated by evaluating the model on the validation set. We would run cross-validation using a predefined splitting mechanism for each learning algorithm that we have. A popular choice in machine learning and statistics is $ K $-fold cross-validation. $ K $-fold cross-validation divides the data into $ K $ disjoint and exhaustive sets referred to as validation sets. For every $ k = 1, \dots , K $ the learning algorithm is trained using all data excluding the $ k $'th validation set. Subsequently, the risk of the fitted model is computed by applying it on the validation set that was held out from training. We obtain an estimate of the true risk of the model by repeating this procedure $ K $ times and then averaging the risks. The model with the lowest risk is then selected as our candidate estimate of the regression function. 

This is the idea behind the cross-validation selector \parencite{laan03}. The discrete super learner \parencite{van2007super} is simply the learner (fitted model) that is created by applying the result of the cross-validation selector to our data. The cross-validation selector, given a library of learning algorithms and our data, applies cross-validation to each algorithm in the library. 

Another method, called the ensemble super learner \parencite{van2007super} seeks to combine the learning algorithms into a single learner by applying a \textit{meta learning algorithm} on the library. One way to fit the ensemble super learner is by taking a weighted average of the each learner, where the weights are chosen to minimize the risk of the ensemble. A prediction made by an ensemble super learner will then be a weighted sum of the predictions made by the individual learners.

As we will demonstrate in this thesis, given a library of learning algorithms, the super learner is effectively the best estimate of the data-generating regression that we can obtain. It is the best in the sense that it can not perform worse than the best learner created from our library in terms of risk. 

To formalize the notion of the best learner in the library, we will define the oracle selector. The oracle selector knows $ P $ -- hence it is called the oracle -- and selects the learner with the lowest risk relative to $ P $. In this thesis we demonstrate that the cross-validation selector is asymptotically equivalent to the oracle selector. 

Our setup and notation is similar to \cite{vaart06} and \cite{laan03}. The focus in the thesis will to demonstrate the effectiveness of the super learner applied to binary regression. More specifically, we focus on the case where we regress on a binary outcome $ Y \in \{0,1\} $. The conditional expectation of $ Y $ given $ X $ exactly becomes the conditional probability $ P(Y = 1 \mid X = x) $. The simulation results in section \ref{sec:simulations} shows that the super learner is able to achieve the minimum risk as the number of training samples increases. The simulation is conducted by first defining the data-generating distribution explicitly, from which we can sample from using standard sampling methods that are available in R. We evaluate a collection of learning algorithms, including logistic regression and XGBoost. From this library, we create the super learner and compare its performance against the individual algorithms.

The choice to focus on binary regression stems from its significance in various fields. For instance, in biomedicine, researchers might want to predict patient mortality upon administering a specific drug. The survival indicator for the patient is a binary outcome, and the regression $ P(Y = 1 \mid X = x) $ could represent the probability of the patient's survival. 

\section{Background}

Our setup and notation is similar to \cite{vaart06} and \cite{laan03}:
Let a statistical model $ \mathcal{P} $ be given on the measurable space $ (\mathcal{O}, \mathcal{A}) $ where $ \mathcal{O} = \{0,1\} \times \mathcal{X} $ is our sample space for some $ \mathcal{X} \subseteq \mathbb{R}^{d} $. 
We will consider the parameter set $ \Theta = \{\btheta \mid \btheta : \mathcal{X} \to [0,1]\} $, which represents the set of \textit{regression functions} that map from our covariates to the probability interval. We define the quadratic loss and the corresponding risk that we wish to minimize.
\begin{definition}[Quadratic loss]
    The quadratic loss or $ L^2 $-loss, $ L : \mathcal{O} \times \Theta \to [0, \infty) $, for an observation $ o \in \mathcal{O} $ and a regression function $ \btheta \in \Theta $ is defined as 
\begin{align*}
    L(o, \btheta) = L((y,x), \btheta) = (y - \btheta(x))^2.
\end{align*}
\end{definition}
A natural aim would be to find the optimal parameter value $\btheta^* \in \Theta$ that minimizes the expected $L^2$-loss, or conditional risk $R: \btheta \to \mathbb{R}$ given by 
\begin{equation} \label{l2risk}
    R(\btheta, P) := \int L(o, \btheta)  dP(o).
\end{equation}
Theorem \ref{minrisk} shows that the minimum risk is achieved by the conditional probability $ x \mapsto P(Y = 1\mid X = x) $; we also say that the quadratic loss is \textit{strictly proper} \parencite{gneiting2007strictly}. 
\begin{theorem} \label{minrisk}
    Let $ (\mathcal{O} , \mathcal{A}, P) $ be a probability space for some probability measure $ P \in \mathcal{P} $. Let $ \Theta $ be the set of regression functions of the form $ \btheta : \mathcal{X} \to [0,1] $. Let the loss function be the $ L^2 $-loss $ L(o, \btheta) = (y - \btheta(x))^2 $, then for the optimum $ \btheta^* $ defined as 
    \begin{align*}
        \btheta^* := \argmin_{\btheta \in \Theta} R(\btheta, P)= \argmin_{\btheta \in \Theta} \int L(o, \btheta)  dP(o),
    \end{align*}
    it holds for an observation $ O = (Y, X) \sim P $ that
    \begin{align*}
        \btheta^{*}(x) = E(Y \mid X = x).
    \end{align*}
\end{theorem}
\begin{proof}
    Proof emulated from \parencite{gyorfi2002distribution}[ch. 1]. 

    Let $ \btheta \in \Theta $ be arbitrary and $ m(x) = E(Y \mid X = x) $, we have 
    \begin{align*}
        E| \theta(X) - Y |^2 &= E| \theta(X) - m(X) + m(X) - Y |^2 \\
                             &= E| \theta(X) - m(X) |^2 + E| m(X) - Y |^2 + 2 E[ (\theta(X) - m(X))(m(X) - Y) ].
    \end{align*}
    We see that the last term is zero, see that by using the tower rule we have 
    \begin{align*}
        E[ (\theta(X) - m(X))(m(X) - Y) ] &= E[ E[ (\theta(X) - m(X))(m(X) - Y) \mid X] ] \\
                                                       &= E[ (\theta(X) - m(X)) E( (m(X) - Y) \mid X) ] \\
                                                       &= E[ (\theta(X) - m(X)) ( m(X) - E(Y \mid X)) ] \\
                                                       &= E[ (\theta(X) - m(X)) ( m(X) - m(X)) ] \\
                                                       &= 0.
    \end{align*}
    We conclude that
    \begin{align*}
        \int L(o, \theta) d P(o) &= E| \theta(X) - m(X) |^2 + E | m(X) - Y |^2.
    \end{align*}
    The first term after the equality is always positive and is $ 0 $ only when $ \theta = m $, this proves that $ m $ minimizes the expression above. 
\end{proof}
It follows immediately that if $ Y $ is binary, then $ E(Y \mid X = x) = P(Y = 1 \mid X = x) $. Our goal is to estimate $ \btheta^{*} $, which means to \textit{learn} the true regression function from our data. We will introduce the terminology \textit{learning algorithm} and \textit{learner} in the context of learning from our data.  
\subsection{Learning algorithms and learners}
We will denote $ O_1 , \dots , O_n \in \mathcal{O} $ and $ D_n = (O_1 , \dots , O_n) $ as our observations and data respectively. 
\begin{definition}[Learning algorithm]
    A learning algorithm is a measurable map $ \la : \mathcal{O}^{n} \to \Theta $ for $ n \in \mathbb{N} $. 
\end{definition}
We will throughout assume that the learning algorithm is well defined for each $ n \in \mathbb{N} $, and that permuting the observations have no effect on the outcome, i.e., the algorithm is symmetric in the observations.  
\begin{definition}[Learner or fitted learner]
    Given a learning algorithm $ \la $. A learner is a stochastic variable in $ \Theta $ representing the outcome of applying the learning algorithm to our data $ \btheta = \la(D_n) $.
\end{definition}
Formally $ \la(D_n) $ is a stochastic variable since $ D_n $ is stochastic. In practice, we would observe $ O_1 = o_1, \dots, O_n = o_n $, following which we can employ our learning algorithm on the observed data. In machine learning, the process of applying an algorithm on the data is usually referred to as \textit{model training} or \textit{fitting}, the outcome of the training is a \textit{trained model} or simply a \textit{fit}. We will refer to the trained model as a learner which naturally depends on our data that we have observed. 

\begin{example}[Parametric and nonparametric learning algorithms]
    An example of a parametric learning algorithm is logistic regression. In logistic regression we assume that the conditional probability, $ P(Y = 1 \mid X = x) $, can be expressed as $ \btheta(x) = \expit( \beta x) $ for some $ \beta \in \mathbb{R}^{d} $. The parameter $ \beta $ can be estimated via maximum likelihood.   

Nonparametric learning algorithms such as gradient boosting -- most notably XGBoost -- can also be used to estimate the regression function. XGBoost has a number of hyperparameters that can be tuned. These parameters include the number of boosted trees, depth of each tree, learning rates, and others. However, the most critical parameter is the internal loss objective which could for example be log-loss or mean squared error. XGBoost aims to iteratively refine the fitted learner by approximating the data $x \mapsto f_m(x)$ at each step $m$. It does so by introducing a new tree $h_m(x)$, which is trained on the error of $ f_m(x)$, such that $f_{m + 1}(x) = f_m(x) + h_m(x)$. The internal loss of the updated learner, $f_{m + 1}$, evaluated on the training data, is lower than that of the previous learner due to the inclusion of the new tree \parencite{chen2016xgboost}.

The parameters of the fitted XGBoost are not directly interpretable. Despite this, XGBoost has demonstrated its ability to model very complex datasets \parencite{chen2016xgboost}.

\end{example}
We will denote the empirical measure obtained from the data $ D_n $ as
\begin{align*}
    P_n(A) = \frac{1}{n} \sum_{i = 1}^{n} \delta_{O_i}(A)\qquad \text{ for } A \in \mathcal{A},
\end{align*}
where $ \delta_{O_i} $ is the Dirac measure over $ O_i $. When the observations are independent and identically distributed, then there is a one-to-one correspondence between the empirical measures obtained from $ n $ observations and $ D_n $. We can, therefore, write $ \la(P_n)$ as an alternative representation of the learner $\la(D_n)$, by adjusting the notation slightly without introducing ambiguity. The motivation for using this notation will become clearer in the subsequent section, where we introduce the discrete super learner.


%\begin{example}[Regression functions $ \btheta $] \label{ex:regfunc}
%    Let $ O_1 = (Y_1 , X_1) ,\ldots, O_n = (Y_n , X_n) \in  \mathcal{O} = \mathbb{R} \times \mathcal{X} $ be i.i.d. observations distributed according to some $ P \in \mathcal{P} $ such that they satisfy the model 
%    \begin{align*}
%        Y_1 = \btheta_0(X_1) + \varepsilon,
%    \end{align*}
%    for an unobservable stochastic error term $ \varepsilon $. The goal is to estimate an unknown \textbf{regression function} $ \btheta_0 \in \Theta $ where $ \btheta = \{\btheta \mid \btheta : \mathcal{X} \to \mathbb{R}\}$, is the set of possible regression functions each having $ \mathcal{X} $ as their domain. \parencite{vaart06}
%\end{example}
%\begin{example}[Parametric statistical model] \label{ex:parametricfam}
%    In the case where we have $ n $-i.i.d. observations distributed according to some data-generating distribution $ P \in \mathcal{P} $, we have that each $ O_i = (Y_i , X_i) $, and $ X_i $ is a stochastic variable. The distribution $ P $ factorizes essentially into two parts, the conditional distribution of $ Y $ given $ X $ and the background distribution of $ X $, so $ P = P_{Y \mid X = x} \cdot P_X $. In this setup we are doing estimation with random design \parencite{gyorfi2002distribution}.  
%
%We can formalize the setup as follows: $ O = (Y, X) \sim P $, if $ Y $ is $ \mathcal{B}(\mathbb{R})-\mathcal{B}(\mathbb{R}) $ measurable and $ X $ is $ \mathcal{F} - \mathcal{B}(\mathbb{R})  $ measurable for some sigma-algebra $ \mathcal{F} $ on $ \mathcal{X} $, then a \textbf{generalized regression model} could be considered as parametrized family of distributions, $ \mathcal{P} = \{P_{\btheta} \mid \btheta \in \Theta\} $, given that $ \btheta $ is finite-dimensional.
%
%    We can parametrize the conditional probability distributions for $ Y_1 $ given $ X_1 = x $ as $ \mathcal{Q} = \{Q_{\btheta(x)} \mid \btheta \in \Theta \} $ such that $ Q_{\btheta(x)} $ is a valid probability distribution on $ \mathcal{B}(\mathbb{R}) $ for each $ x \in X $ and $ \btheta \in \Theta $. For a given $ P_{\btheta} \in \mathcal{P} $ there will exist a $ Q_{\btheta} \in \mathcal{Q} $ such that  
%    \begin{align*}
%        P_{\btheta}(Y \in A \mid X = x) = Q_{ \btheta(x)}(A) \qquad \text{for all } A \in \mathcal{B}(\mathbb{R}).
%    \end{align*}
%    If we assume that $ X_1 $ is distributed according to some $ H_0 $ on $ \mathcal{X} $, then the distribution $ P_{\btheta} $ over our observations (the joint over $ Y $ and $ X $) will be
%    \begin{align*}
%        P_{\btheta }(X \in A, Y \in B ) = \int_{A} Q_{\btheta(x)}(B) d H_{0}(x) 
%    \end{align*}
%    for every $ A \in \mathcal{F} $ and $ B \in \mathcal{B}(\mathbb{R}) $. 
%\end{example}
%\begin{example}[Logistic regression model] \label{logregmod}
%    Let $ O_1 = (Y_1 , X_n) , \ldots, O_n = (Y_n , X_n) \in \mathcal{O} = \{0,1\} \times \mathcal{X} $ be i.i.d. observations from some distribution $ P_{\btheta_0} \in \mathcal{P} $, where $ Y_i $ is binary and $ \mathcal{X} \subseteq \mathbb{R}^{k} $. We would like to estimate the parameter function $ \btheta_{0} \in \Theta $
%\begin{align*}
%    \btheta_0(x) = E(Y_1 \mid X_1 = x) = P_{\btheta_0}(Y_1 = 1 \mid X_1 = x),
%\end{align*}
%In logistic regression we assume that $ \btheta = \{x \mapsto \expit(\beta x) \mid \beta \in \mathbb{R}^{k}\} $, so $ \btheta_0(x) = \expit(\beta_0 x) $, then the goal becomes to estimate the $ k $-dimensional parameter $ \beta_0 $, in this case the $ \mathbb{R}^{k} $ parameter $ \beta_0  $ completely determines $ \btheta_0 $, so $ \btheta $ is also $ k $-dimensional. % I am not sure if the last is true
%
%The conditional distributions of $ Y_1 $ given $ X_1 = x $ are Bernoulli distributions and can be parametrized as $ \mathcal{Q} = \{\operatorname{Ber}(\expit(\beta x )) \mid \beta \in \mathbb{R}^{k}  \}  $. Now from example \ref{ex:parametricfam} we know that the statistical model, $ \mathcal{P} $, can be parametrized through $ \beta $, in particular we have 
%\begin{align*}
%    P_{\beta}(Y_1= 1 , X_1 \in A) &= \int_{A} Q_{\btheta(x)}(\{1\}) dH_{0}(x)\\
% &= \int_{A} \expit(\beta x )  d H_0(x) 
%\end{align*}
%If $ H_{0} $ has density $ f $ w.r.t. Lebesgue measure, we can write
%\begin{align*}
%    P_{\beta}(Y_1= 1 , X_1 \in A) &= \int_{A} \expit(\beta x ) f(x)  d m(x) 
%\end{align*}
%\end{example}
%We will now turn our attention to statistical estimators. Statistical literature commonly write that an estimator is stochastic variable taking values in our parameter space $ \hat{\btheta} \in \Theta  $. An estimator is achieved by considering i.i.d. observations $ O_1 , \ldots, O_2 \in\mathcal{O} $ distributed according to some measure $ P $ from some statistical model $ \mathcal{P} $. We leave the model unspecified as it can be both parametric or nonparametric. Now let $ h : \mathcal{O}^{n} \to \btheta $ be a measurable map, an estimator created from $ h $ is the random variable $ T = h(O_1 , \ldots, O_n) $. For $ \btheta \subseteq \mathbb{R}^{k} $ the canonical $ \sigma $-algebra on $ \btheta $ is the Borel algebra, but when the parameter set is a set of functions, the $ \sigma $-algebra can only be chosen after careful consideration of constraints on $ \btheta $. 


\section{The Discrete Super Learner}
\subsection{Library of learners}
We would now like to consider the scenario where we have a set of learning algorithms, $ \la_1, \ldots, \la_\ml $. From these algorithms, we can define the \textit{library of learning algorithms} 
$$ \lib = \{\la_{\q} \mid 1 \leq \q \leq \ml \}, $$
of size $ \ml $. Our natural goal is to find $ \tilde{\la} \in \lib $ such that $ \tilde{\la}(P_n) $ that achieves the lowest risk among our learners 
$$ \tilde{\la}  = \argmin_{\la \in \lib} R(\la(P_n) , P). $$
However, we cannot compute the risk of a learner directly as it depends on $ P $. We seek to provide an estimate for $ \tilde{\la} $ which we will denote as $ \hat{\la} $. 

\subsection{Cross-validation methodology}
To provide the estimate $ \hat{\la} $ we will proceed via cross validation. Cross-validation randomly splits our data into a \textit{training set} and a \textit{test set} that our algorithms are trained and evaluated on. Let the random binary vector -- referred to as the \textit{split variable} or just \textit{split} -- $ \Sn = (\Sn_1,\ldots,\Sn_n) \in \{0,1\}^{n} $ be independent of $ D_n $ such that $ \Sn_i = 0 $ indicates that $ O_i $ should be in the training set and $ \Sn_i = 1 $ indicates that $ O_i $ belongs to the test set. The split $ \Sn $ depends on $ n $ as it is formally a tuple in $ \{0,1\}^{n} $, here the superscript $ n $ is used to indicate that. We can define the empirical distributions over these two subsets, $ P_{n,\Sn}^0$ and $ P_{n,\Sn}^{1} $ as
\begin{align*}
    P_{n,\Sn}^{0} &= \frac{1}{n_0} \sum_{i: \Sn_i = 0} \delta_{O_i} \\
    P_{n,\Sn}^{1} &= \frac{1}{n_1} \sum_{i: \Sn_i = 1} \delta_{O_i},
\end{align*}
where $ n_1 = \sum_{i = 1}^{n} \Sn_i, n_0 = 1-n_1$ identifies the number of observations in the test and training set respectively.

\begin{example}[Random splits] \label{splits}
   For $ n $ observations we have $ 2^{n} $ ways of choosing which observations should be in the training set and in the test set. It might not be desirable to define the discrete probabilities for $ \Sn $ over $ \{0,1\}^{n} $ simply as $ \frac{1}{2^{n}} $ for each possible combination of training/test data, since that would also include the combination where $ n_1 = 0 $. To ensure that there is always a certain amount of observations in our test set, let $ 0 < n_1 < n $ be given, we see that there are $ \begin{pmatrix}
        n \\ n_1
    \end{pmatrix}$ ways of choosing both the test and training set. We can therefore define the distribution of $ \Sn $ as 
    \begin{align*}
        P \left(\Sn = s^n \right) = \begin{pmatrix}
            n \\ n_1
    \end{pmatrix}^{-1} \qquad \text{for each } s^n \in \{0,1\}^{n} \text{ where } \sum_{i} s^n_i = n_1,
    \end{align*}
    this procedure is also known as Monte Carlo cross-validation \parencite{laan03}. In practice one would have a hard time validating over all $ \begin{pmatrix} n \\ n_1\end{pmatrix} $ combinations as it can be very large, for $ n = 100 $ and $ n_1 = 10 $ corresponding to $ 10\% $ of the data being in the validation set, the number of combinations is around $ 1.7 \cdot 10^{13} $. One would instead approximate by randomly sampling possible combinations. However, by doing random sampling there is the possibility that some observations will not be in any of the sampled validation sets. The probability of this happening will become lower as we sample more times.  
\end{example}
\subsection{K-fold cross validation}
The idea behind $ K $-fold cross-validation is to split the data into $ K $ equally sized folds, where the candidate learners are fitted on $ K - 1 $ folds and their performance is evaluated on the remaining fold. We index the folds by $ s \in \{1, \dots , K\} $ and let $ s(i) $ indicate the fold that observation $ i $ belongs to. We will let $ P_{n, s}^{0} $ denote the empirical measure over the observations that are not in fold $ s $ -- the training set, and let $ P_{n, s}^{1} $ denote the empirical measure over the observations that are in the fold $ s $ -- the validation set. 

$ K $-fold cross-validation amounts to looking at a single split variable $ \Sn $ whose probability mass is equally distributed on the binary vectors indexed by $ s $. In practice one would select a $ K $ and use a random splitting procedure to generate $ K $ binary vectors such that the entire dataset is the disjoint union of the folds specified by each vector. It is also possible to run $ K $-fold cross-validation multiple times which amounts to multiple calls to the random splitting procedure.
\subsection{Risks and selectors}
We now provide the formal definitions for the expected loss associated with our learners. Recall that the expected $L^2$-loss (\ref{l2risk}) was the integral of the loss with respect to data-generating distribution $P$. Consider a learner or regression function $ \theta $. Upon observing our data $D_n$, we can define the empirical risk as the integral of the loss function with respect to $P_n$, as follows
\begin{align*}
    R(\btheta , P_n) = \int L(o, \btheta) d P_{n}(o). 
\end{align*}
For example, if $\Sn$ is a split for our data $D_n$, the risk of our learner on the cross-validation test data can be expressed as
 \begin{align*}
     R(\theta, P_{n, \Sn}^{1}) =  \int L(o, \btheta) d P_{n,\Sn}^{1}(o).
 \end{align*}
 The following definitions are analogous to what is stated in section 1 and 2 of  \parencite{laan03}.
 \begin{definition}[Expected loss averaged over splits \parencite{vaart06}]
     Given the data $D_n$, a split-variable $\Sn$ and a library $ \lib $. Let $ \la \in \lib $ be a learning algorithm. The expected loss for the learner, $\la(P_{n,\Sn}^0)$, created from applying $ \la $ on the training data, averaged over $ \Sn $ is  
    \begin{align*}
        E_{\Sn} R( \la(P_{n,\Sn}^{0}), P),
    \end{align*}
    where $ P $ is the data-generating distribution.
\end{definition}
The expectation $ E_{\Sn} $ is a simple average since $ \Sn $ is discrete. Therefore, for a given $ \la $ we have 
\begin{align*}
    E_{\Sn} R( \la(P_{n,\Sn}^{0}), P) &= \sum_{s^{n} \in \{0,1\}^{n}} R(\la(P_{n, s^{n}}^{0}), P) \cdot P(\Sn = s^{n}). 
\end{align*}


\begin{definition}[Oracle selector \parencite{laan03}]
    Given the data $ D_n $, a split variable $ \Sn $ and a library $ \lib $, the learning algorithm $ \la \in \lib $ with the lowest averaged expected loss is the \textit{oracle selected algorithm} for $ n $ observations
    \begin{align*}
        \tilde{\la}_n := \argmin_{\la \in \lib} E_{\Sn} R(\la(P_{n,\Sn}^0 ) , P).
    \end{align*}
\end{definition}
Estimating $ \tilde{\la}_n $ via cross-validation amounts to replacing $ P $ with $ P_{n, \Sn}^{1} $ in the second argument of $ R $.
\begin{definition}[Cross-validation expected loss]
     Given the data $D_n$, a split-variable $\Sn$ and a library $ \lib $. Let $ \la \in \lib $ be a learning algorithm. The cross-validation expected loss for the learner, $\la(P_{n,s}^0)$, created from applying $ \la $ on the training data, averaged over $ \Sn $ is  
    \begin{align*}
        E_{\Sn} R( \la(P_{n,\Sn}^{0}), P_{n, \Sn}^{1}),
    \end{align*}
    where $ P_{n,\Sn}^{1} $ is the test data as specified by the split-variable.
\end{definition}

\begin{definition}[Cross-validation selector \parencite{laan03}] \label{def:cvselector}
    Given the data $ D_n $, a split variable $ \Sn $ and a library $ \lib $, the learning algorithm $ \la \in \lib $ with the lowest cross-validation risk is the \textit{cross-validation selected algorithm} for $ n $ observations
    \begin{align*}
        \hat{\la}_n := \argmin_{\la \in \lib} E_{\Sn} R(\la(P_{n,\Sn}^0 ) , P_{n, \Sn}^{1}).
    \end{align*}
\end{definition}
%We are interested in the risk difference between the cross-validation selector and and the oracle selector, we remark that the optimal risk is attained at the true value $ \btheta_0 $ 
%\begin{align*}
%    R(\btheta_0) = \int L(o, \btheta_0)  dP(o),
%\end{align*}
%and clearly it is the case that $ R(\btheta_0) \leq R( \btheta  ) $ for any learner $ \btheta $ of $ \btheta_0 $.
%Given a set of learners we define the centered conditional risk as the difference 
%\begin{align*}
%    \Delta_{S}( \btheta_{ \hat{\q} }, \btheta_0 ) &= R( \btheta _{ \hat{\q} }(P_{n, S}^{0})) -R(\btheta_0) \\
%                                                       &= E_{S} \int L(o, \btheta_{ \hat{\q} }(P_{n, S}^{0})) - L(o, \btheta_0) dP(o) 
%\end{align*}
%
The oracle selector and cross-validation selector are simply what chooses a learning algorithm from the library. Alternatively, one can first define the oracle selector as the index, $ \tilde{\q} $, of the algorithm in the library with the lowest risk, then the oracle selected algorithm would naturally be $ \la_{ \tilde{\q} } $. 

\subsection{Discrete super learner}
\begin{definition}[Discrete super learner]
    The \textit{discrete super learner}, $ \hat{\la}_n(P_n) $, created from a library $ \lib $ is the cross-validation selected algorithm fitted to the entire dataset 
    \begin{align*}
        \mathcal{X} \ni x \mapsto \hat{\la}_n(P_n)(x). 
    \end{align*}
\end{definition}
Formally, the map above is a random map as $ P_n $ is stochastic. The discrete super learner is not a specific learner among the learners in the library, but the result of applying the cross-validation selector to the library. 

\subsection{Oracle property}
We introduce the notation $Pf$ for the integral $\int f dP$ of an integrable function $f$ with respect to $P$. Additionally, if $P_n$ represents the empirical measure of $O_1, \dots, O_n$, we denote the empirical process indexed over an appropriate class of functions $\mathcal{F}$ as $G_n f = \sqrt{n}(P_n f - P f)$. Furthermore, we extend this notation to $G_{n, \Sn}^{i} f = \sqrt{n}(P_{n, \Sn}^{i} - Pf)$ for the empirical processes that correspond to applying the empirical measure over either the training data or test data, $ i = 0 $ or $ i = 1 $.

In the following results we assume that a proper loss function $ L: \mathcal{O} \times \Theta \to \mathbb{R} $ has been given.  
\begin{lemma}[Lemma 2.1 in \parencite{vaart06}] \label{finitesampledecomp}
    Let $ G_{n} $ be the empirical process of an i.i.d. sample of size $ n $ from the distribution P and $ \lib$ a library of learning algorithms. Furthermore, let $ \hat{\la}_n  $ and $ \tilde{\la}_n $ denote the cross-validation- and oracle selected algorithms from $ \lib $ respectively. For $ \delta > 0 $ it holds that
   \begin{align*}
       E_{\Sn} &R( \hat{\la}_n(P_{n, \Sn}^{0}), P) \leq (1 + 2 \delta) E_{\Sn} R( \tilde{\la}_n(P_{n, \Sn}^{0}) , P ) \\ 
                                                                &+E_{\Sn} \frac{1}{\sqrt{n_1} } \max_{\la \in \lib} \int L(o, \la(P_{n, \Sn}^{0})) d ((1 + \delta) G_{n,\Sn}^{1} - \delta \sqrt{n_1} P)(o)  \\
                                                                &+E_{\Sn} \frac{1}{\sqrt{n_1} } \max_{\la \in \lib} \int-L(o, \la(P_{n, \Sn}^{0})) d ((1 + \delta) G_{n,\Sn}^{1} + \delta \sqrt{n_1} P)(o).
   \end{align*}
\end{lemma}
\begin{proof}
    See appendix.
\end{proof}
To control the bounds for the expected loss we introduce Bernstein pairs 
\begin{definition}[Bernstein pair \parencite{vaart06}]
    Given a measurable function $ f: \mathcal{O} \to \mathbb{R} $, the tuple $ (M(f) , v(f)) $ is a Bernstein pair if 
    \begin{equation} \label{bernstein}
        M(f)^2 P\left( e^{|f|/M(f)} -1 - \frac{|f|}{|M(f)|}\right) \leq \frac{1}{2}v(f). 
    \end{equation}
\end{definition}
\begin{proposition} \label{unifbernstein}
    If $ f $ is uniformly bounded, then $ (\norm{f}_{\infty}, \frac{3}{2} Pf^2 ) $ is a Bernstein pair. 
\end{proposition}
\begin{proof}
    Following proof is due to \parencite{vaart06}[ch. 8.1]. 
    \begin{align*}
        \norm{f}_{\infty}^2 P\left( e^{|f|/\norm{f}_{\infty}} -1 - \frac{|f|}{\norm{f}_{\infty}}\right) &= \norm{f}_{\infty}^{2} \sum_{k \geq 2}^{\infty} P \frac{|f|^{k}}{\norm{f}^{k}_{\infty}k!} = Pf^2 \sum_{k \geq 2}^{\infty} P\frac{|f|^{k-2}}{\norm{f}^{k-2}_{\infty} k! } \\
                                                                                                        &\leq P f^2 \sum_{k \geq 2}^{\infty} \frac{\norm{f}_{\infty}^{k-2}}{\norm{f}^{k-2}_{\infty} k! } = P f^2 \sum_{k \geq 2}^{\infty} \frac{1}{k !}\\
                                                                                                        &= P f^2 (e-2)\leq \frac{3}{4} P f^2 = \frac{1}{2} \left(\frac{3}{2} Pf^2\right). 
    \end{align*}
   In the first inequality we replace the absolute value of $ f $ with the uniform norm, which is larger. 
\end{proof}

\begin{example}[Binary regression] \label{ex:bernsteinexample}
    Consider binary regression with quadratic loss. Let $ \btheta \in \Theta $ be arbitrary, then a Bernstein pair for the function $ f_{\btheta}(o) = L(o, \btheta) = (Y - \theta(X))^2$ can be found by applying proposition \ref{unifbernstein}. By requiring that $ \theta(x) \in [0,1] $, then it is clear that $ f $ is bounded between $ 0 $ and $ 1 $ for all $ o \in \mathcal{O} $ since $ Y \in \{0,1\} $. We also note that the Bernstein pair for $ f_{\btheta} $ satisfies $ 0 \leq \norm{f_\theta}_{\infty} \leq 1 $ and $ 0 \leq \frac{3}{2}Pf_{\btheta}^2 \leq 1 $, so they are bounded. 

\end{example}


\begin{lemma}[Lemma 2.2 in \parencite{vaart06}] \label{finitesamplebound}
    Let $G_{n}$ be the empirical process of an i.i.d. sample of size $n$ from the distribution $P$ and assume that $P f \geq 0$ for every $f \in \mathcal{F}$ in some set of measurable functions $ \mathcal{F} $ on $ \mathcal{O} $. Then, for any Bernstein pair $(M(f), v(f))$ and for any $\delta>0$ and $1 \leq p \leq 2$,
    \begin{align*}
        E_{D_n} \max_{f \in \mathcal{F}}(G_n-\delta \sqrt{n} P) f \leq \frac{8}{n^{1 / p-1 / 2}} \log (1+\# \mathcal{F}) \max _{f \in \mathcal{F}}\left[\frac{M(f)}{n^{1-1 / p}}+\left(\frac{v(f)}{(\delta P f)^{2-p}}\right)^{1 / p}\right].
    \end{align*}
    The same upper bound is valid for $ E_{D_n} \max_{f \in \mathcal{F}}(G_n+\delta \sqrt{n} P)(-f) $. 
\end{lemma}
The expectation $ E_{D_n} $ is taken wrt. the joint probability measure over our observations, here we have that $ D_n \sim P_{O}^{n} = P_{O_1} \otimes P_{O_2} \otimes \dots \otimes P_{O_n} $. 

\begin{theorem}[Theorem 2.3 in \parencite{vaart06}] \label{finitesample}
   Let $ \lib $ be a library of learning algorithms of size $ \ml $. For $ \btheta \in \Theta $ let the numbers $ (M(\btheta) , v(\btheta)) $ be a Bernstein pair for the function $ o \mapsto L(o, \btheta) $ and assume that $ R(\btheta, P) \geq 0 $ for every $ \btheta \in \Theta $. Then for $ \delta > 0 $ and $ 1 \leq p \leq 2 $ it holds that 
   \begin{align*}
       E_{D_n} E_{\Sn} &R(\hat{\la}_n(P_{n, \Sn}^{0}), P) \leq(1 + 2 \delta) E_{D_n} E_{\Sn} R( \tilde{\la}_n(P_{n,\Sn}^{0}), P) +\\
                       &(1 + \delta) E_{\Sn} \left(  \frac{16}{n_1^{1/p}} \log (1 +k) \sup_{\btheta \in \Theta} \left[ \frac{M(\btheta)}{n_1^{1-1/p}} +  \left( \frac{v(\btheta)}{R(\btheta, P)^{2-p}} \right)^{1/p} \left( \frac{1 + \delta}{\delta} \right)^{2/p-1} \right]\right).
   \end{align*}
   Where $ \hat{\la}_n $ and $ \tilde{\la}_n $ are the cross-validation- and the oracle selected algorithms from $ \lib $. 
\end{theorem}
In the expectations above, we are taking the expectation wrt. the random split-variable $ \Sn $ as well as the joint distribution of our observations. In a more verbose manner one would write 
\begin{align*}
    E_{D_n} E_{\Sn}  R(\hat{\la}_n(P_{n, \Sn}^{0}), P) = \int R(\hat{\la}_n(P_{n, \Sn}^{0}), P) d (P_{\Sn} \otimes  P^{n}_O ).
\end{align*}

\begin{proof}[Proof]
    We will apply lemma \ref{finitesamplebound} to the second and third terms on the left hand side of the inequality in lemma \ref{finitesampledecomp}. Let $ \mathcal{F} = \{o \mapsto L(o, \la(P_{n,\Sn}^{0})) \mid \la \in \lib\}$, be the collection of functions obtained by applying the loss $ L $ to each algorithm in our library $ \lib$. Note that $ \mathcal{F} \subseteq \{o \mapsto L(o, \btheta) \mid \btheta \in \Theta\} $, and since $ R(\btheta, P) \geq 0 $ for every $ \btheta \in \Theta $ it follows that $ Pf \geq 0 $ for every $ f \in \mathcal{F} $.

First, we take the expectation wrt. $ D_n $ on both sides in lemma \ref{finitesampledecomp}. For the second term we have 
\begin{align*}
&E_{D_n} E_{\Sn} \frac{1}{\sqrt{n_1} } \max_{\la \in \lib} \int L(o, \la(P_{n, \Sn}^{0})) d ((1 + \delta) G_{n,\Sn}^{1} - \delta \sqrt{n_1} P)(o)\\
&= 
E_{D_n}E_{\Sn} \frac{1 + \delta}{\sqrt{n_1} } \max_{\la \in \lib} \int L(o, \la(P_{n, \Sn}^{0})) d (G_{n,\Sn}^{1} - \frac{\delta }{1 + \delta} \sqrt{n_1} P)(o)\\
&=E_{\Sn} \frac{1 + \delta}{\sqrt{n_1} } E_{D_n}\max_{\la \in \lib} \int L(o, \la(P_{n, \Sn}^{0})) d (G_{n,\Sn}^{1} - \frac{\delta }{1 + \delta} \sqrt{n_1} P)(o).
\end{align*}
Where we use Fubini in the last equality. Recall that $ \Sn \indep D_n $, so we can always consider $ n_1 $ as fixed given $ D_n $, now applying lemma \ref{finitesamplebound} to the expression above with $ n = n_1 $ yields 
\begin{align*}
&E_\Sn\frac{1 + \delta}{\sqrt{n_1} } E_{D_n} \max_{\la \in \lib} \int L(o, \la(P_{n, \Sn}^{0})) d (G_{n,\Sn}^{1} - \frac{\delta }{1 + \delta} \sqrt{n_1} P)(o) \\
&\leq E_{\Sn} \frac{1 + \delta}{\sqrt{n_1}} \frac{8}{n_1^{1/p-1/2}} \log(1 + k) \max_{\la \in \lib} \left[ \frac{M(\la(P_{n,\Sn}^{0}))}{n_1^{1-1/p}} + \left( \frac{v(\la(P_{n,\Sn}^0) )}{( \frac{\delta}{1 + \delta} )^{2-p} R(\la(P_{n,S}^{0}), P)^{2-p}} \right)^{1/p} \right] \\
&\leq E_{\Sn}\frac{1 + \delta}{\sqrt{n_1}} \frac{8}{n_1^{1/p-1/2}} \log(1 + k) \sup_{\btheta \in \Theta} \left[ \frac{M(\btheta)}{n_1^{1-1/p}} + \left( \frac{v(\btheta)}{( \frac{\delta}{1 + \delta} )^{2-p} R(\btheta,P)^{2-p}} \right)^{1/p} \right] \\
&= (1 + \delta) E_{\Sn}\frac{8}{n_1^{1/p}} \log(1 + k) \sup_{\btheta \in \Theta} \left[ \frac{M(\btheta)}{n_1^{1-1/p}} + \left( \frac{v(\btheta)}{R(\btheta,P)^{2-p}} \right)^{1/p}\left( \frac{1 + \delta}{\delta}  \right)^{2/p-1} \right].  
\end{align*}
Where for the third inequality we take the $ \sup $ over $ \Theta $. We can also bound the third term with the same expression above as lemma \ref{finitesamplebound} is also valid for $ -L $. It is now immediate from lemma \ref{finitesampledecomp} that 
\begin{align*}
    E_{D_n} E_{\Sn} &\int L(o, \hat{\la}_n(P_{n, \Sn}^{0})) d P(o) \leq (1 + 2 \delta) E_{D_n} E_{\Sn} \int L(o, \tilde{\la}_n(P_{n, \Sn}^{0})) d P(o) \\
                    &+ (1 + \delta) E_{\Sn}\frac{8}{n_1^{1/p}} \log(1 + k) \sup_{\btheta \in \Theta} \left[ \frac{M(\btheta)}{n_1^{1-1/p}} + \left( \frac{v(\btheta)}{R(\btheta)^{2-p}} \right)^{1/p}\left( \frac{1 + \delta}{\delta}  \right)^{2/p-1} \right]\\
                    &+ (1 + \delta) E_{\Sn}\frac{8}{n_1^{1/p}} \log(1 + k) \sup_{\btheta \in \Theta} \left[ \frac{M(\btheta)}{n_1^{1-1/p}} + \left( \frac{v(\btheta)}{R(\btheta)^{2-p}} \right)^{1/p}\left( \frac{1 + \delta}{\delta}  \right)^{2/p-1} \right].
\end{align*}
The second and third terms above are identical, meaning that they can be combined into one term where the numerator in the first fraction is $ 16 $ instead of $ 8 $. 
\end{proof}

\begin{remark}
One might notice that $ \Sn $ still appears in the remainder of the bound above, but the only variable in the remainder that depends on $ \Sn $ is $ n_1 $. It is, therefore, completely possible to drop the expectation wrt. $ \Sn $ if $ n_1 $ is deterministic, for example, if it is a fraction of $ n $.  
\end{remark}


In the following corollary we assume that the Bernstein pairs $ (M(\btheta), v(\btheta)) $ are bounded over $ \Theta $, thus taking the supremum over $ \Theta $ will not result in the remainder being infinite. The assumption is indeed valid for binary regression as we have commented in example \ref{ex:bernsteinexample}. 
\begin{corollary}[Asymptotic equivalence] \label{cor:dslasymptoticequivalence}
    If there exists an $ \varepsilon > 0 $ such that 
   \begin{align*}
       E_{D_n} E_{\Sn} R(\tilde{\la}_n(P_{n, \Sn}^{0}), P) > \varepsilon \qquad \text{for all } n \in \mathbb{N},
   \end{align*}
   and if $ n_1 \to \infty $ as $ n \to \infty $, then the risk of the super learner is asymptotically equivalent to the risk of the oracle selected learner, that is
   \begin{align*}
       \lim_{n \to \infty} \frac{E_{D_n} E_{\Sn} R(\hat{\la}_n(P_{n, \Sn}^{0}), P)}{E_{D_n} E_{\Sn} R(\tilde{\la}_n(P_{n, \Sn}^{0}), P)} = 1.
   \end{align*}
\end{corollary}
For the sake of simplicity and the previous remark we will consider $ n_1 $ as a sequence of numbers polynomial in $ n $, this will allow us to remove $ E_{\Sn} $ in the remainder term. 
\begin{proof}
    We let $ \delta $ depend on $ n $, define $ \delta_n := 1/\log(n) $.  By choosing $ p = 1 $ in theorem \ref{finitesample} and substituting $ \delta $ with $ \delta_n $, we obtain
    \begin{align*}
        E_{D_n} E_{\Sn} R(\hat{\la}_n(P_{n, \Sn}^{0}), P) &\leq(1 + 2 \delta_n) E_{D_n} E_{\Sn} R(\tilde{\la}_n(P_{n,\Sn}^{0}), P)\\
                                                        &+(1 + \delta_n) \frac{16}{n_1} \log (1 +k) \sup_{\btheta \in \Theta} \left[ M(\theta) + \frac{v(\theta)}{R(\theta, P)} \cdot \frac{1 + \delta_n}{\delta_n}  \right]\\
                                                        &= (1 + 2 \delta_n) E_{D_n} E_{\Sn} R(\tilde{\la}_n(P_{n,\Sn}^{0}), P)\\
                                                        &+(1 + \delta_n) \frac{16}{n_1} \log (1 +k) \sup_{\btheta \in \Theta} \left[ M(\theta) + \frac{v(\theta)}{R(\theta, P)} \cdot (\log(n) + 1) \right].
    \end{align*}
    In the above inequality the remainder term goes to $ 0 $ as $ n_1 $ increases. The $ \log(n) $ term in the supremum will be dominated by the $ 1/n_1 $ that is multiplied in the front. By dividing the oracle risk on both sides, we obtain
    \begin{align*}
        \frac{E_{D_n} E_{\Sn} R(\hat{\la}_n(P_{n, \Sn}^{0}), P)}{E_{D_n} E_{\Sn} R(\tilde{\la}_n(P_{n,\Sn}^{0}), P)} 
        &\leq 1 + 2 \delta_n\\
        &+\frac{(1 + \delta_n)\frac{16}{n_1} \log (1 +k) \sup_{\btheta \in \Theta} \left[ M(\theta) + \frac{v(\theta)}{R(\theta , P)} \cdot (\log(n) + 1) \right]}{E_{D_n} E_{\Sn} R( \tilde{\la}_n(P_{n,\Sn}^{0}), P)}.
    \end{align*}
    The fraction on the right hand side will converge to $ 0 $ as $ n \to \infty $, since $ n_1 $ is polynomial in $ n $ and the oracle risk is bounded away from $ 0 $ by assumption. The entire right hand side will therefore converge to $ 1 $ as $ \delta_n \to 0 $. Note also that by the definition of the oracle, we have 
    \begin{align*}
        E_{D_n} E_{\Sn} R(\tilde{\la}_n(P_{n,\Sn}^{0}), P) \leq E_{D_n} E_{\Sn} R(\tilde{\la}_n(P_{n,\Sn}^{0}), P),
    \end{align*}
    implying that 
        \begin{align*}
            1 \leq \frac{E_{D_n} E_{\Sn} R(\hat{\la}_n(P_{n, \Sn}^{0}), P)}{E_{D_n} E_{\Sn} R(\tilde{\la}_n(P_{n,\Sn}^{0}), P)}, 
    \end{align*}
    applying the squeeze theorem thus yields the desired result.         
\end{proof}
In the proof of the asymptotic result we have chosen $ \delta_n = 1/\log(n) $, in doing so, the remainder converges to $ 0 $ at a rate of $ \log(n)/n $. An immediate consequence of this is that if we have a library of parametric learning algorithms and one of them corresponds to the true regression, then the oracle risk might very well converge at a rate of $ 1/n $, implying that the risk of the discrete super learner achieves an almost parametric rate of convergence of $ \log(n)/n $ to the oracle \parencite{van2007super}.  
For the asymptotic equivalence to hold, we must have that $ n_1 \to \infty $ as $ n \to \infty $. This is a reasonable assumption for cross-validation schemes such as $ K $-fold cross-validation or if $ n_1 = np $ for $ p \in (0,1) $ where the test size increases with the amount of available data. Note that the condition is not satisfied for leave-one-out cross-validation since $ n_1$ is equal to $ 1 $ for every $ n $.  

\begin{example}[Regression]
   In a regression context with quadratic loss, the risk for a learner can never be zero unless the outcome is deterministic given $ x $. We may begin by noting that the risk of $ \theta^*(x) = E(Y \mid X = x) $ is always positive,
   \begin{align*}
       \int (Y - E(Y \mid X))^2 d P \geq 0. 
   \end{align*}
   Since the integrand is positive, the integral is zero if and only if $ Y = E(Y \mid X) $ almost surely. That is only the case when $ Y $ is deterministic given $ X $, i.e. $ Y $ is $ X $-measurable. Assuming, therefore, that $ Y $ is not deterministic given $ X $, then we note that by the fact that the quadratic loss is strictly proper due to theorem \ref{minrisk}, it holds for any $ \theta \in \Theta $ that
   \begin{align*}
       R(\theta, P) \geq R(\theta^*, P) > 0,
   \end{align*}
    thus showing that the risk of any learner is strictly positive.
\end{example}



% \subsubsection{Choosing $p = 1$}
% \begin{align*}
%    ER(\btheta_{\hat{\q}}(P_{n, S}^{0})) &\leq(1 + 2 \delta) ER(\btheta_{ \tilde{\q}}(P_{n,S}^{0})) +(1 + \delta) E \left(  \frac{16}{n_1} \log (1 +k) \sup_{\btheta \in \Theta} \left[ M(\btheta) + \frac{v(\btheta)}{R(\btheta)} \frac{1 + \delta}{\delta}\right]\right)
% \end{align*}
% In the equation provided, we observe that we can manipulate the following variables: sample size $n$, validation set size $n_1$, parameter $\delta > 0$, and the number of learners $k$. Assuming $k$ remains constant, the validation set size $n_1$ could be either stochastic, depending on the split variable $S$, or fixed as a constant, as illustrated in example \ref{splits}. For instance, we can set $n_1 = n/2$. By establishing a fixed value for $n_1$, we can drop the expectation in the second term. 

% The supremum on the left side of the equation might increase significantly because $R(\btheta)$ could be very small. However, by carefully selecting the value of $v(\btheta)$, we can avoid the fraction from growing too large. Note that for any $ \btheta \in \Theta $:
% \begin{align*}
%     \frac{v(\btheta)}{R(\btheta)} = \frac{3}{2} \frac{\int L(o, \btheta)^2 d P(o)}{\int L(o, \btheta) dP(o)  } \leq \frac{3}{2} \frac{\int L(o, \btheta) \cdot 1 dP(o) }{\int L(o, \btheta) dP(o) } = \frac{3}{2},
% \end{align*}
% by using $ 0 \leq L(o, \btheta) \leq 1 $ almost surely. Since $ M(\btheta) $ is constant for all $ \btheta \in \Theta $, it is possible to drop the supremum. Combining all the information above we obtain 
% \begin{align*}
%    ER(\btheta_{\hat{\q}}(P_{n, S}^{0})) &\leq(1 + 2 \delta) ER(\btheta_{ \tilde{\q}}(P_{n,S}^{0})) +(1 + \delta) \frac{16}{n_1} \log (1 +k) \left[ 1 + \frac{3}{2} \frac{1 + \delta}{\delta}\right]\\
%                                         &= (1 + 2 \delta) ER(\btheta_{ \tilde{\q}}(P_{n,S}^{0})) +\log (1 +k) \frac{3 + 5\delta}{2\delta}(1 + \delta) \frac{16}{n_1} \\
%                                         &= (1 + 2 \delta) ER(\btheta_{ \tilde{\q}}(P_{n,S}^{0})) +\log (1 +k) \frac{3 + 8\delta + 5 \delta^2}{2\delta}\frac{16}{n_1} \\\
%                                         &= (1 + 2 \delta) ER(\btheta_{ \tilde{\q}}(P_{n,S}^{0})) +\log (1 +k) \left( \frac{3}{2\delta} + 4 + \frac{5}{2} \delta \right) \frac{16}{n_1},
% \end{align*}
% We can now adjust for the precision in our bound by choosing $ \delta $ and $ n $. Note that a small delta will mean that the first term $ (1 + 2 \delta) E R(\btheta_{ \tilde{q} }(P_{n, S}^{0})) $ will become smaller, but this is at the expense that the remainder term becomes larger due to the $ \frac{1 + \delta}{ \delta} $ fraction at the end. By choosing $ n $ to be large, we can partially compensate for a smaller delta.

% We might, therefore, for each $ n $, choose the $ \delta_n $ that minimizes the left-hand side for the given $ n $. By substituting $ n_1 = n/2 $ into the left hand side expression and then expanding it we obtain 
% \begin{align*}
%  ER(\btheta_{ \tilde{\q}}(P_{n,S}^{0})) + 2 \delta ER(\btheta_{ \tilde{\q}}(P_{n,S}^{0})) +\frac{3}{2\delta}\log (1 +k) \frac{32}{n} + 4\log (1 +k) \frac{32}{n} + \frac{5\delta}{2} \log (1 +k)   \frac{32}{n},
% \end{align*}
% we observe that when $n$ is fixed, two terms remain constant, specifically the first and fourth terms, as they do not depend on $\delta$. The optimal $\delta_n$ can be determined as follows:
% \begin{align*}
%     \delta_n &= \argmin_{\delta} \frac{1}{\delta} \cdot \frac{3 \cdot 32}{2n} \log(1 + k) + \delta \cdot \left(2ER(\btheta_{ \tilde{\q} }(P_{n, S}^{0})) + \frac{5 \cdot 32}{2n} \log(1 + k) \right) \\
%     &= \argmin_{\delta} \frac{1}{\delta} \cdot \frac{48}{n} \log(1 + k) + \delta \cdot \left(2ER(\btheta_{ \tilde{\q} }(P_{n, S}^{0})) + \frac{80}{n} \log(1 + k) \right) \\
%     &= \argmin_{\delta} \frac{1}{\delta} a(n) + \delta b(n),
% \end{align*}
% Essentially, solving for the minimum is a convex optimization problem, with the terms $a(n) = \frac{48}{n} \log(1 + k)$ and $b(n) = 2ER(\btheta_{ \tilde{\q} }(P_{n, S}^{0})) + \frac{80}{n} \log(1 + k)$ remaining constant with respect to $n$. By differentiating the expression above and setting it equal to zero we obtain 
% \begin{align*}
%     0 &= \left( \frac{1}{\delta} a(n) + \delta b(n) \right)' = - \frac{1}{\delta^2} a(n) + b(n),
% \end{align*}
% and so we obtain the optimum by isolating $ \delta $ 
% \begin{align*}
%     \delta_n = \sqrt{\frac{a(n)}{b(n)}}. 
% \end{align*}
% We see that $ a(n) $ converges at a rate of order $ 1/n $ to 0 and the rate of $ b(n) $ depends on the unknown oracle risk. For $ n \to \infty $, the risk term $ 2ER(\theta_{ \tilde{q} }(P_{n, S}^{0})) $ converges to the lowest possible risk achievable by any of the learners, it might be the case that the convergence is fast, i.e. it converges at a rate of order $ 1/n^{-1/2} $. That is if  
% \begin{align*}
%     \delta_n = \sqrt{\frac{a(n)}{b(n)}} = \sqrt{\frac{\frac{48}{n} \log(1 + k)}{O(\frac{1}{\sqrt{n}}) + \frac{80}{n} \log(1 + k) }} = \sqrt{\frac{48 \log(1 + k)}{O(\sqrt{n}) + 80 \log(1 + k)}}
% \end{align*}


%Nonetheless, obtaining a useful bound could be challenging unless $n$ is considerably large. In that case, the oracle risk might be extremely close to the true risk. Unless the remainder constitutes only a minuscule portion of the oracle risk, the finite sample result may not be of significant interest.
%
%To illustrate the problem, consider a concrete example where we might model $ ER(\btheta_{ \tilde{\q} }(P_{n, S}^{0})) $ as a simple function, $ f(n) $, that is monotonically decreasing for $ n \to \infty $. For example
%\begin{align*}
%    f(n)= \frac{1}{2\sqrt{n}} \qquad \text{for }  n \in \mathbb{N} 
%\end{align*}
%which steadily decreases to $ 0 $, but is bounded by $ 0.5 $. 
%
%We now have the choice of solving for $ \delta $ given $ n $, or vice-versa. We might for example be interested in our cross-validation risk to be at most $ 2\% $ more than our oracle risk, which amounts to selecting $ \delta = 0.01 $, we will then solve for $ n $ for an acceptable size on the remainder. For some $ \gamma \in (0, 1) $ which represents the tolerable proportion of our oracle risk that our remainder term can at most attain, and $ \delta = 0.01$, we wish to solve  $ n $ in the following 
%\begin{align*}
%    1.02  \gamma \cdot f(n) \geq \log (1 + k) \frac{154.025 \cdot 16 }{n_1} =  \log (1 + k) \frac{154.025 \cdot 16 }{\frac{n}{2}} = \log (1 + k) \frac{4928.8}{n},
%\end{align*}
%which is equivalent to 
%\begin{align*}
%    \frac{\log (1 + k) \cdot 4928.8}{1.02 \gamma } \leq f(n) \cdot n = \frac{n}{2 \sqrt{n}} = \frac{\sqrt{n}}{ 2} .
%\end{align*}
%For $ k = 5 $ and $ \gamma = 0.01 $, the left hand side is around 865806, multiplying this number by 3 and then squaring it yields us $n = 2998480118544 $, at this point, the oracle risk is $ f(n) \approx 5 \cdot 10^{-26}$. 

%The following result is due to \parencite{laan03}: 
%\begin{theorem}[Asymptotic equality]
%    The cross validation selector $ \hat{\q} $ performs asymptotically as well as the oracle selector $ \tilde{\q} $ in the sense that 
%    \begin{align*}
%        \frac{\Delta_{S}( \btheta_{ \hat{\q} } , \btheta_0 )}{ \Delta_{S}( \btheta_{ \tilde{\q} } , \btheta_0) } \to 1 \qquad \text{ in probability for } n \to \infty
%    \end{align*}
%\end{theorem}


\section{The Ensemble Super Learner}
The idea behind the \textit{ensemble super learner} is to combine the predictions made by each candidate learner, $\la_1(P_{n}), \ldots, \la_k(P_{n}) $, into a single prediction. The idea of combining learners was examined in \cite{breiman1996stacked} which considered the construction of an ensemble learner by forming a linear combination of the learners in the library. The method of forming a linear combination of learners is known as \textit{stacked regression} or \textit{stacking}, and is a variant of the ensemble super learner. The ensemble super learner does not restrict to taking a linear combination of the learners but that we are able to apply a \textit{meta learning algorithm} on the \textit{level 1 data}. The ensemble super learner can be thought of as a general method of balancing the weaknesses of each learner by forming an ensemble. However, as we will see, $ K $-fold cross-validation is integral in forming the ensemble super learner. 

We will now proceed to give the definitions and setup necessary to define the ensemble super learner.
\subsection{Level 1 data}
Let $ D_n $ be our data and let $ \lib $ be a library of learning algorithms. The \textit{level 1 covariates} of $ \lib $ applied to $ D_n $ as specified by a $ K $-fold cross validation procedure is  
$$ \mathcal{Z} = \{Z_i = (\la_1(P_{n, s(i)}^{0})(X_i), \ldots, \la_\ml(P_{n, s(i)}^0)(X_i) ) \}_{i = 1}^{n} \subseteq [0,1]^{k}, $$
which is the set of possible outcomes by applying the learners on the observed $ X_i $'s. 

\begin{definition}[Level 1 data]
    The \textit{level 1 data} is given by concatenating our observed $ Y_i $'s with the level 1 covariates, i.e.
    \begin{align*}
        \lone_{n}  = \{(Y_i ; Z_i) = (Y_i; \la_1(P_{n, s(i)}^{0})(X_i), \ldots, \la_\ml(P_{n, s(i)}^{0})(X_i)) \}_{i = 1}^n \subseteq \{0,1\} \times \mathcal{Z}. 
    \end{align*}
\end{definition}
The level 1 data is exactly the predictions made by the learners on the observations that they were not trained on. Thus, \textit{level 0 data} is what we refer to as $ D_n $. 
We now define $ \mathcal{M} $ as the class of all measurable functions that map from $ \mathcal{Z} $ to $ [0,1] $ which we will refer to as \textit{meta learners}. 
\subsection{Meta learners}
\begin{definition}[Meta learner]
    The \textit{meta learner} is a function $ \meta: \mathcal{Z} \to [0,1] $ in $ \mathcal{M} $ that maps the output of the candidate learners to a prediction. 
\end{definition}
The goal is to estimate the regression of $ Y $ given the predictions made by our learners, $ \mathcal{Z} \ni z \mapsto E(Y \mid Z = z) $, which is an element in $ \mathcal{M} $. We accomplish this by applying a \textit{meta learning algorithm} to the level 1 data. 
\begin{definition}[Meta learning algorithm]
    A \textit{meta learning algorithm} $ \Meta $ is a measurable map that creates a meta learner from our level 1 data $ \lone_{n} \mapsto \Meta(\lone_{n}) \in \mathcal{M} $. 
\end{definition}
A meta learning algorithm seeks to estimate $ E (Y \mid Z) $. It can for example be a parametric learning algorithm such as logistic regression. If a parametric learning algorithm is used, it is important to take into account that the level 1 covariates $ Z_i $ are highly correlated and that multicollinearity can occur. \cite{breiman1996stacked} suggests ridge regression as a possible way to shrink the coefficients. Another method is to use a non-negativity constraint, where we specify that the coefficients must sum to 1 in the case of a simple linear combination. 

\subsection{Ensemble super learner}
\begin{definition}[Ensemble super learner \parencite{van2007super}]
    Let a library of learning algorithms $ \lib $ be given and let $ \lone_{n} $ be the level 1 data obtained by fitting each learning algorithm according to some $ K $-fold cross validation procedure. Let $ \meta = \Meta(\lone_{n}) $ be the outcome of applying a meta learning algorithm to the level 1 data, then the map 
    \begin{align*}
       x \mapsto \esl(P_n)(x) = \meta(\la_1(P_{n})(x), \ldots, \la_k(P_{n})(x) ),
    \end{align*}
    is called the \textit{ensemble super learner} and we will denote it by $ \esl(P_{n}) $. 
\end{definition}
Here the $ P_n $ indicates that each learner $ \la \in \lib $ is fitted on the entire data set. A meta learner, $ \meta $, is used to combine the predictions made by each learner. 


\subsection{Oracle property}
The meta learning algorithm, $ \Meta $, fits the learner that most accurately predicts $ Y $ given $ Z $, a way of formalizing the notion of `fitting' is that $ \Meta $ seeks to select the meta learner with the lowest risk among a indexed set of meta learners. Let $ \mathcal{A} $ be a parameter set (for example Euclidean), for each $ a \in \mathcal{A} $ let $ \meta_{a} $ be a meta learner. In a practical setting, one might consider $ \meta_{a} $ as the learner obtained by instantiating it with the parameter $ a $. Given our level 1 data $ \lone_{n} $, define 
\begin{align} \label{eq:meta_learning_algorithm}
    \hat{a} := \argmin_{a \in \mathcal{A}} \frac{1}{n} \sum_{i = 1}^{n} L((Y_i, Z_i), \meta_{a}) = \argmin_{a \in \mathcal{A}} \frac{1}{n} \sum_{i = 1}^{n} (Y_i - \meta_{a}(Z_i))^2.
\end{align}
The meta learning algorithm applied to $ \lone_{n} $ returns $ \meta_{ \hat{a} } $, which is the meta learner with the empirical lowest risk on the level 1 data. We will now consider $ \mathcal{A}_n \subseteq \mathcal{A} $ such that $ \ml(n) = |\mathcal{A}_{n}| $ depends on $ n $. Denote the ensemble super learner that uses the meta learner $ \meta_{ a } $ as 
\begin{align*}
    \esl_{a}(P_{n}) := \meta_{a}(\la_1(P_{n}), \ldots, \la_k(P_{n})).
\end{align*}
Denote the cross-validation- and oracle selector of $ a $ as 
\begin{align*}
    \hat{a}_n &:= \argmin_{a \in \mathcal{A}_{n}} \frac{1}{K} \sum_{s = 1}^{K} R(\esl_{a}(P_{n, s}^{0}), P_{n, s}^{1}), \\
    \tilde{a}_n &:= \argmin_{a \in \mathcal{A}_n} \frac{1}{K} \sum_{s = 1}^{K} R(\esl_{a}(P_{n, s}^{0}) , P),
\end{align*}
where $ P $ is the distribution of the observations in our level 0 data. Note that for each $ s' $ up to $ K $ 
$$ \esl_a(P_{n, s'}^{0})(X_i) = \meta_{a}(\la_{1}(P_{n, s'}^{0})(X_i) , \dots , \la_{\ml}(P_{n, s'}^{0})(X_i) ) = \meta_{a}(Z_i). $$ 
holds true for each $ i $ if $ s(i)$ equals $ s' $, i.e. when $ X_i $ is in the validation set $ s' $. By using the fact that the $ K $ folds are disjoint and exhaustive, we can express $ \hat{a}_n $ as  
\begin{align} \label{eq:meta_learning_algorithm_cv}
    \hat{a}_n = \argmin_{a \in \mathcal{A}_n} \frac{1}{K} \sum_{s' = 1}^{K} \frac{K}{n} \sum_{i: s(i) = s'} (Y_i - \esl_{a}(P_{n,s'}^{0}) (X_i))^2 = \argmin_{a \in \mathcal{A}_n} \frac{1}{n} \sum_{i = 1}^{n} (Y_i - \meta_{a}(Z_i))^2.
\end{align}
Equation (\ref{eq:meta_learning_algorithm_cv}) shows that the cross-validation selector, $ \hat{a}_n $, is exactly the objective of the meta learning algorithm defined in \eqref{eq:meta_learning_algorithm}. 

The way that $ \hat{a}_n  $ and $ \tilde{a}_n $ are defined fit directly into our existing framework for the discrete super learner. Recall that the cross-validation selector selected the algorithm that had the lowest cross-validation risk averaged across some split variable $ \Sn $. By using a $ K $-fold cross-validation scheme, the averaged risk over $ \Sn $ becomes the averaged risk over the $ K $ folds, which is exactly the above definition of $ \hat{a}_n $. In the case of the ensemble super learner, the learning algorithm, $ \meta_{a} $, is determined by the parameter $ a $, so the selection problem reduces to selecting the most optimal parameter over a finite parameter set $ \mathcal{A}_n $. The parameter set $ \mathcal{A}_n $ indexes the meta learning algorithms, and is what the cross-validation selector acts upon. It is, therefore, possible to think of $ \mathcal{A}_n $ as approximating the entire parameter set $ \mathcal{A} $ using a grid of points such that the approximation becomes finer as the number of points increase. The key difference lies in the fact that the ensemble super learner is a completely new learner, whereas the discrete super learner is an algorithm selected from the library $ \lib $ fitted on the dataset.

The following finite sample oracle inequality is a direct consequence of the oracle inequality for the discrete super learner, theorem \ref{finitesample}.
\begin{theorem}[Finite sample oracle inequality]
    Let $ \lib $ be a library of learning algorithms of size $ k $. Let $ \esl_{ \hat{a}_{n} } $ be the ensemble super learner over $ \lib $ obtained by applying the cross-validation selector over a finite parameter space $ \mathcal{A}_n $ using the $ K $-fold cross-validation scheme. Let $ \esl_{ \tilde{a}_n } $ be the oracle ensemble learner obtained by applying the oracle selector over the same set of parameters. If the conditions of theorem \ref{finitesample} are satisfied, then the following oracle inequality holds
    \begin{align*}
        \frac{1}{K} \sum_{s = 1}^{K} E_{D_n} R(\esl_{ \hat{a}_n }(P_{n, s}^{0}) , P) &\leq (1 + 2 \delta) \frac{1}{K} \sum_{s = 1}^{K} E_{D_n} R(\esl_{ \tilde{a}_n }(P_{n,s}^{0}), P) \\
                                                                                     &+ (1 + \delta) \frac{16 K}{n} \log(1 + k(n)) \sup_{\btheta \in \Theta} \left[ \frac{M(\btheta) K}{n} + \frac{v(\btheta)}{R(\btheta)} \frac{1 + \delta}{\delta} \right].  
    \end{align*}
\end{theorem}
\begin{proof}
    Replace $ E_{\Sn} $ in theorem \ref{finitesample} with the average over the $ K $ folds and $ n_1$ with $n/K $, then by using $ p = 1 $ the desired result is obtained.  
\end{proof}

\begin{corollary}[Asymptotic equivalence]
    If the number of points in the parameter sets $ \mathcal{A}_n $ grows at most at a polynomial rate, that is $ k(n) \leq n^{p} $ for some $ p \in \mathbb{N} $, then the ensemble super learner $ \esl_{ \hat{a}_n } $ is asymptotically equivalent to the oracle ensemble learner $ \esl_{ \tilde{a}_n } $, i.e.
    \begin{align*}
        \lim_{n \to \infty} \frac{\frac{1}{K} \sum_{s = 1}^{K} E_{D_n} R(\esl_{ \hat{a}_n }(P_{n, s}^{0}) , P)}{\frac{1}{K} \sum_{s = 1}^{K} E_{D_n} R(\esl_{ \tilde{a}_n }(P_{n,s}^{0}), P)} = 1.
    \end{align*}
\end{corollary}
\begin{proof}
    The $ \log(1 + k(n)) $ term is less than $ 1 + \log(k(n)) = 1 + p \log(n) $, which is of lower order than $ \sqrt{n} $, thus the remainder term goes to zero as $ n \to \infty $.
\end{proof}

\subsection{Choice of meta learning algorithm} \label{simplex}
Given the level 1 data, $ \lone_n $, the meta learning algorithm seeks to optimize over the parameter set $ \mathcal{A}_n \subseteq \mathcal{A} $. The interpretation of $ \mathcal{A} $ depends on the chosen learning algorithm. If a parametric learning algorithm is used, for example logistic regression, then $ \mathcal{A} $ is $ \mathbb{R}^{\ml} $ where $ \ml $ is the number of learners in $ \lib $. The learner $ \meta_a $ for $ a \in \mathcal{A} $ will then be the map $ z \mapsto \expit(az) $.
\cite{van2007super} mentions that it is also possible to use a non-parametric learning algorithm as the meta learning algorithm, or even apply a super learning algorithm on the level 1 data. In this case $ \mathcal{A} $ could be measurable functions from $ \mathcal{Z} $ to $ [0, 1] $. The learner $ \meta_a $ for $ a \in \mathcal{A} $ will then be the map $ z \mapsto a(z) $.

\subsubsection{Constrained regression and the $ \ml $-simplex} 
Here we examine a simple meta learning algorithm which is to do a constrained regression on the level 1 data. The reason for examining this particular meta learning algorithm is because the discrete super learner is a special case of the ensemble super learner that utilizes this algorithm. The goal is to fit a weighted combination of the learners, such that the weights sum to $ 1 $. We can achieve this by parametrizing $ \mathcal{A} $ as the $ \ml $-simplex given by
\begin{align*}
    \mathcal{A} = \left\{ a \in \mathbb{R}^{\ml} \mid \sum_{\q = 1}^{\ml} a_\q = 1 , a_\q \geq 0 \text{ for } 1 \leq \q \leq k \right\}. 
\end{align*}
The meta learner $ \meta_{a} $ for $ a \in \mathcal{A} $ is then the map $ z \mapsto z \cdot a $. By Cauchy-Schwarz we have $ z \cdot a \leq \norm{z} \norm{a} \leq 1 $ meaning that the range of $ \meta_a $ is $ [0,1] $. The ensemble super learner that applies $ \meta_a $ is, therefore, a valid learner in $ \Theta $. To apply the oracle results, we will cover $ \mathcal{A} $ with equidistant points on the simplex surface. For every $ n $, we specify $ \mathcal{A}_n \subseteq \mathcal{A} $ such that the number of points in $ \mathcal{A}_n $ increases with $ n $ at a polynomial rate, and that for any $ a \in \mathcal{A} $ we have 
\begin{align*}
    \min_{b_n \in \mathcal{A}_n } \norm{a - b_n} \leq \frac{1}{n}.
\end{align*}
This is clearly possible by figure \ref{fig:simplex}. It thus follows that we can find a sequence of points $ b_n \in \mathcal{A}_n $ such that $ \meta_{b_n} \to \meta_{a} $ for any $ a \in \mathcal{A}$. The conclusion is that if $ \tilde{a} $ specified the learner $ \meta_{ \tilde{a} } $ with the minimum risk, then by applying the cross-validation selector and letting $ n \to \infty $, the meta learner adopted by the ensemble super learner will eventually converge to $ \meta_{ \tilde{a} } $. 

\begin{figure}[H]
    \centering
    \includestandalone[width=0.5\textwidth]{figures/simplex-3.tex}
    \caption{The parameter set $ \mathcal{A} $ as a $ \ml $-simplex for $ \ml = 3 $. Here $ \mathcal{A}_n $ is visualized as points on the surface.}
    \label{fig:simplex}
\end{figure}

The discrete super learner is essentially a special case of the ensemble super learner that utilizes constrained regression. If $ a_\q = 1 $ for any $ 1 \leq \q \leq k $ then the ensemble super learner predicts the same as the $ \q $'th learner. The discrete super learner can, therefore, be seen as constrained regression optimizing for $ a $ over the vertices of the $ k $-simplex. Consequently we conclude that the ensemble super learner that uses constrained regression must outperform the discrete super learner at minimizing risk since it minimizes over the entire $ k $-simplex.  


\begin{remark}
In reality the optimization procedure that is available on our hardware can only optimize over finite subset of $ \mathcal{A} $. The finite subset is determined by the floating-point precision of the processor, and perhaps also the internal parameters of the optimization procedure itself. For example, gradient descent algorithms usually have a hyperparameter such as `step size' or `learning rate' which controls how large of a step the algorithm takes in the direction of the minimum. The parameter set $ \mathcal{A}_n $ which depends on $ n $ also does not correspond to our actual optimization procedure. We do not choose some grid of points in the parameter set that depends on the number of observations such that for each point the risk is evaluated and then the minimum is picked. The minimum is optimized over all feasible points, for example if the risk is convex, then the minimum can sometimes be solved explicitly or there will exist procedures that guarantee convergence to the minimum. 
\end{remark}

\subfile{simulations}
\newpage

\section{Appendix}
\subsection{Proof of lemma \ref{finitesampledecomp}}
\begin{proof}
    We first note that the minimizing property of $ \hat{\la}_n $ (definition \ref{def:cvselector}) implies that 
    \begin{align}
        E_{\Sn} R( \hat{\la}_n(P_{n, \Sn}^{0}), P_{n, \Sn}^{1} ) \leq E_{\Sn} R( \tilde{\la}_n(P_{n, \Sn}^{0}) , P_{n, \Sn}^{1}  ). \label{eq:minizing} 
    \end{align}
   We can write (\ref{eq:minizing}) as  
   \begin{align*}
       E_{\Sn} R( \hat{\la}_n(P_{n, \Sn}^{0} )&, P_{n, \Sn}^{1}) \leq (1 + 2 \delta) E_{\Sn} R( \tilde{\la}_n(P_{n, \Sn}^{0}) , P ) \\ 
               &+E_{\Sn} \frac{1}{\sqrt{n_1} } R(\tilde{\la}_n (P_{n, \Sn}^{0}), (1 + \delta) G_{n,\Sn}^{1} - \delta \sqrt{n_1} P)\\
               &-E_{\Sn} \frac{1}{\sqrt{n_1} } R(\hat{\la}_n(P_{n, \Sn}^{0}), (1 + \delta) G_{n,\Sn}^{1} + \delta \sqrt{n_1} P)  .
   \end{align*}
  Indeed, by examining the risk in the second term on the right hand we obtain 
  \begin{align*}
      R&(\tilde{\la}_n (P_{n, \Sn}^{0}), (1 + \delta) G_{n,\Sn}^{1} - \delta \sqrt{n_1} P)\\
                    &= (1 + \delta) R(\tilde{\la}_n (P_{n, \Sn}^{0}), G_{n,\Sn}^{1}) - \delta \sqrt{n_1} R(\tilde{\la}_n (P_{n, \Sn}^{0}), P) \\ 
                    &= (1 + \delta) \left[\sqrt{n_1} R(\tilde{\la}_n (P_{n, \Sn}^{0}), P_{n,\Sn}^{1}) - \sqrt{n_1} R(\tilde{\la}_n (P_{n, \Sn}^{0}), P)\right] - \delta \sqrt{n_1} R(\tilde{\la}_n (P_{n, \Sn}^{0}), P)\\ 
                    &= (1 + \delta) \sqrt{n_1} R(\tilde{\la}_n (P_{n, \Sn}^{0}), P_{n,\Sn}^{1}) - (1 + 2\delta)\sqrt{n_1}   R(\tilde{\la}_n (P_{n, \Sn}^{0}), P).
  \end{align*}
  And for the third term 
  \begin{align*}
     R&(\hat{\la}_n(P_{n, \Sn}^{0}), (1 + \delta) G_{n,\Sn}^{1} + \delta \sqrt{n_1} P)\\
       &= (1 + \delta) R(\hat{\la}_n (P_{n, \Sn}^{0}), G_{n,\Sn}^{1}) + \delta \sqrt{n_1} R(\hat{\la}_n (P_{n, \Sn}^{0}), P) \\ 
       &= (1 + \delta) \left[\sqrt{n_1} R(\hat{\la}_n (P_{n, \Sn}^{0}), P_{n,\Sn}^{1}) - \sqrt{n_1} R(\hat{\la}_n (P_{n, \Sn}^{0}), P)\right] + \delta \sqrt{n_1} R(\hat{\la}_n (P_{n, \Sn}^{0}), P)\\ 
       &=(1 + \delta) \sqrt{n_1} R(\hat{\la}_n (P_{n, \Sn}^{0}), P_{n,\Sn}^{1}) - \sqrt{n_1}   R(\hat{\la}_n (P_{n, \Sn}^{0}), P).
  \end{align*}
  The $ \sqrt{n_1} $ in each term will disappear after we multiply with $ \frac{1}{\sqrt{n_1}} $. We now add the first and second term on the right hand side 
  \begin{align*}
      &(1 + 2 \delta) E_{\Sn} R( \tilde{\la}_n(P_{n, \Sn}^{0}) , P ) +E_{\Sn} \frac{1}{\sqrt{n_1} } R(\tilde{\la}_n (P_{n, \Sn}^{0}), (1 + \delta) G_{n,\Sn}^{1} - \delta \sqrt{n_1} P)\\ 
      &= (1 + 2 \delta) E_{\Sn} R( \tilde{\la}_n(P_{n, \Sn}^{0}) , P ) + E_{\Sn} \left[(1 + \delta) R(\tilde{\la}_n (P_{n, \Sn}^{0}), P_{n,\Sn}^{1}) - (1 + 2\delta)R(\tilde{\la}_n (P_{n, \Sn}^{0}), P) \right]\\
      &=(1 + \delta) E_{\Sn} R(\tilde{\la}_n (P_{n, \Sn}^{0}), P_{n,\Sn}^{1}).
  \end{align*}
Now by combining all terms on the right hand side we get
\begin{align*}
    &(1 + \delta) E_{\Sn} R(\tilde{\la}_n (P_{n, \Sn}^{0}), P_{n,\Sn}^{1})\\
    &-E_{\Sn} \frac{1}{\sqrt{n_1} } \left[ (1 + \delta) \sqrt{n_1} R(\hat{\la}_n (P_{n, \Sn}^{0}), P_{n,\Sn}^{1}) - \sqrt{n_1}   R(\hat{\la}_n (P_{n, \Sn}^{0}), P) \right] \\
    &= (1 + \delta) E_{\Sn} R(\tilde{\la}_n (P_{n, \Sn}^{0}), P_{n,\Sn}^{1})- (1 + \delta)E_{\Sn}R(\hat{\la}_n (P_{n, \Sn}^{0}), P_{n,\Sn}^{1})+ E_{\Sn} R(\hat{\la}_n (P_{n, \Sn}^{0}), P).
\end{align*}
By the minimizing property the difference between the second and first term must be positive. Since we are adding a positive number to $ E_{\Sn} R(\hat{\la}_n (P_{n, \Sn}^{0}), P) $, it follows that it must be less than whatever is on the right. In the final lemma we replace $ \hat{\la}_n  $ and $ \tilde{\la}_n  $ by the maximum over $ \lib $.  
  
\end{proof}


\newpage
\printbibliography
\end{document}
