\documentclass[11pt, a4paper]{article}
\usepackage[english, science, hyperref]{ku-frontpage}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{mdframed}
\usepackage{mathtools}

\setlength\parindent{0pt}
\DeclareMathOperator*{\argmax}{arg\,max} 
\DeclareMathOperator*{\argmin}{arg\,min}

\newtheorem{theorem}{Theorem}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{remark}{Remark}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\theoremstyle{remark}
\newtheorem{example}{Example}

\setlength\arraycolsep{2 pt}
\setcounter{tocdepth}{2}
\setcounter{secnumdepth}{2}
\newcommand{\cl}{q}

\assignment{A Bachelor of Science thesis}
\author{Jinyang Liu}


\title{Super Learners}
\subtitle{and their oracle properties}
\date{Submitted: \today}
\advisor{Supervised by Prof. Thomas Gerds\\ Co-supervised by Prof. Niels Richard Hansen \\Department of Mathematical Sciences \\University of Copenhagen, Denmark}
% \frontpageimage{example.png}

\begin{document}
\begingroup
    \fontencoding{T1}\fontfamily{lmr}\selectfont
    \maketitle
    \tableofcontents
    \newpage
\endgroup


\section{Introduction}
Let $ X_1, ..., X_n $ be $ n $-i.i.d. observations distributed according to $ P \in \mathcal{P} $ on some measurable space $ (\mathcal{X}^{n}, \mathcal{A}) $ where $ X_{i} \in \mathcal{X} $ for each $ i $ and $ \mathcal{P} $ is our statistical model. For a parameter set $ \Theta \subseteq \mathbb{R}^{p}$ we define the corresponding loss function $ L : \mathcal{X} \times \Theta \to [0, \infty) $ as a measurable map such that our goal is to find an estimator $ \hat{\theta}  $ that minimizes the true risk function $ R: \Theta \to \mathbb{R} $ given as
\begin{align*}
    R(\theta) = \int L(x, \theta)  \mathrm{d}P(x) = EL(X_1) 
\end{align*}
\begin{definition}[Estimator of $ \theta_0 $]
    An estimator for $ \theta_0 \in \Theta $ is a measurable map $ \hat{\theta} : \mathcal{X}^{n} \to \Theta  $. In the context where the estimator is fitted from our observations, we write $ \hat{\theta}(P_n) : \mathcal{X}^{n} \to \Theta $ to denote the estimator fitted on the empirical distribution, $ P_n $, of our observations. This estimator is sometimes known as the \textbf{plug-in} estimator for $ \theta $.  
\end{definition}

% I would like to formulate this better 
%\begin{example}[The true estimator $ \theta_0 $]
%    If we have knowledge of the true distribution $ P $ of our observations, then we can define the true estimator $ \theta_0(P) : \mathcal{X}^{n} \to \Theta $ simply as the constant function which that outputs the true value $ \theta_0 $ for any input in $ \mathcal{X}^{n} $.
%\end{example}
%

We would like to consider a set estimators $ \{ \hat{\theta}_{\cl}(P_n) | 1 \leq \cl \leq p \} $, where we find $ \hat{\theta}_{ \hat{\cl} }(P_n) $, which denotes the estimator that minimizes $ R $ and $ \hat{\cl}  $ may depend on the observations. 

In order to find $ \hat{\cl}  $ we have to proceed via cross validation. In cross validation, we randomly split our data into a training set and a test set. Let $ S = (S_1,...,S_n) \in \{0,1\}^{n} $ independent of $ X_1,..., X_n $ such that $ S_i = 0 $ indicates that $ X_i $ should be in the training set and $ S_i = 1 $ indicates that $ X_i $ belongs to the test set. We can define the empirical distributions over these two subsets, $ P_{n,S}^0$ and $ P_{n,S}^{1} $ as
\begin{align*}
    P_{n,S}^{0} &= \frac{1}{n_0} \sum_{i: S_i = 0} \delta_{X_i} \\
    P_{n,S}^{1} &= \frac{1}{1-n_0} \sum_{i: S_i = 1} \delta_{X_i} 
\end{align*}
Where $ n_0 $ would be the number of $ S_i $'s that are marked $ 0 $. 

\begin{definition}[True risk of $ \cl $'th estimator averaged over splits]
    Given the data $ X \in \mathcal{X}^{n} $ and a set of estimators $ \{ \hat{\theta}_{\cl} \mid 1 \leq \cl \leq p \}, p \in \mathbb{N} $. The risks of these estimator averaged over the splits specified by some $ S $ is given as a function of $ \cl $ 
    \begin{align*}
        \cl \mapsto E_S \int L(x, \hat{\theta}_{\cl}(P_{n,S}^{0}) ) \mathrm{d}P(x) = E_S R( \hat{\theta}_\cl(P_{n,S}^{0})) 
    \end{align*}
    Where $ P $ is the true distribution for our data $ X $.
\end{definition}

\begin{definition}[Oracle selector]
    The oracle selector is a function $ \tilde{\cl}: \mathcal{X}^{n} \to \{1,...,p\} $  which finds the estimator that minimizes the true risk given our data $ X \in \mathcal{X}^{n}$. 
    \begin{align*}
        \tilde{\cl}(X) = \argmin_{1 \leq \cl \leq p} E_S R( \hat{\theta} _\cl (P_{n,S}^0 )) 
    \end{align*}
    Where $ P_{n ,s}^{0} $ is the empirical distribution over the training set of $ X $ as specified by some split-variable $ S $. 
\end{definition}
In light of the above definitions, we will define the cross-validation risk and the cross-validation selector for our estimators

\begin{definition}[Cross-validation risk of $ i $'th estimator averaged over splits]
    Given the data $ X \in \mathcal{X}^{n} $ and a set of estimators $ \{ \hat{\theta}_{\cl} \mid 1 \leq \cl \leq p \}, p \in \mathbb{N} $. The cross-validation risks of these estimator averaged over the splits specified by some $ S $ is given as a function of $ \cl $ 
    \begin{align*}
        \cl \mapsto E_S \int L(x, \hat{\theta}_{\cl}(P_{n,S}^{0}) ) \mathrm{d}P_{n, s}^{1}(x) = E_S \hat{R}( \hat{\theta}_\cl(P_{n,S}^{0})) 
    \end{align*}
    Where $ P_{n,S}^{1} $ is the empircal distribution over the validation set for our data $ X $. We write $ \hat{R} $ for empirical risk over the validation set. 
\end{definition}

\begin{definition}[Cross-validation selector]
    The cross-validation selector is a function $ \hat{\cl}: \mathcal{X}^{n} \to \{1,...,p\} $  which finds the estimator that minimizes the cross-validation risk given our data $ X \in \mathcal{X}^{n} $. 
    \begin{align*}
        \hat{\cl}(X) = \argmin_{1 \leq \cl \leq p} E_S \hat{R} ( \hat{\theta} _\cl (P_{n,S}^0 )) 
    \end{align*}
    Where $ \hat{R}  $ is the empirical risk over the validation set and $ P_{n ,s}^{0} $ is the empirical distribution over the training set of $ X $ as specified by some split-variable $ S $. 
\end{definition}
We are interested in the risk difference between the cross-validation selector and and the oracle selector, we remark that the optimal risk is attained at the true value $ \theta_0 $ 
\begin{align*}
    R(\theta_0) = \int L(x, \theta_0)  \mathrm{d}P(x),
\end{align*}
and clearly it is the case that $ R(\theta_0) \leq R( \hat{\theta}  ) $ for any estimator $ \hat{\theta} $ of $ \theta_0 $.
Given a set of estimators we define the centered conditional risk as the difference 
\begin{align*}
    \Delta_{S}( \hat{\theta}_{ \hat{\cl} }, \theta_0 ) &= R( \hat{\theta} _{ \hat{\cl} }(P_{n, S}^{0})) -R(\theta_0) \\
                                                       &= E_{S} \int L(x, \hat{\theta}_{ \hat{\cl} }(P_{n, S}^{0})) - L(x, \theta_0) \mathrm{d}P(x) 
\end{align*}

\begin{theorem}[Asymptotic equality]
    The cross validation selector $ \hat{\cl}  $ performs asymptotically as well as the oracle selector $ \tilde{\cl} $  in the sense that 
    \begin{align*}
        \frac{\Delta_{S}( \hat{\theta}_{ \hat{\cl} } , \theta_0 )}{ \Delta_{S}( \hat{\theta}_{ \tilde{\cl} } , \theta_0) } \to 1 \qquad \text{ in probability for } n \to \infty
    \end{align*}
\end{theorem}


\section{The discrete super learner, dSL}

\subsection{Finite sample properties}

\section{The ensemble super learner, eSL}
\section{Simulation results}
\section{Discussion}
Pellentesque tincidunt sodales risus, vulputate iaculis odio dictum vitae. Ut ligula tortor, porta a consequat ac, commodo non risus. Nullam sagittis luctus pretium. Integer vel nibh at justo convallis imperdiet sit amet ut lorem. Sed in gravida turpis. Vestibulum ante ipsum primis in faucibus orci luctus et ultrices posuere cubilia Curae; Sed in massa vitae ligula pellentesque feugiat vitae in risus. Cras iaculis tempus mi, sit amet viverra nulla viverra pellentesque.

\end{document}
