\documentclass{beamer}
\usepackage{tikz}
\usepackage{caption}
\usepackage[
    backend=biber,
    bibstyle=numeric,
    citestyle=authoryear,
    sorting=ynt,
    natbib
]{biblatex}
\addbibresource{references.bib} % This command is used instead of \bibliography{references.bib}

\usetikzlibrary{positioning}
\usetikzlibrary{calc}
\usetikzlibrary{shapes, positioning, arrows.meta, fit}
\captionsetup{justification=centering}

\usetheme{Copenhagen}
\usecolortheme{default}
\setbeamertemplate{navigation symbols}{}

\DeclareMathOperator*{\argmax}{arg\,max} 
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\indep}{\perp \!\!\! \perp}
\newcommand\norm[1]{\lVert#1\rVert}
\newcommand{\q}{q}
\newcommand{\ml}{k}
\newcommand{\btheta}{\theta}
\newcommand{\la}{\psi}
\newcommand{\Sn}{S^n}
\newcommand{\lib}{\Psi}
\newcommand{\lone}{\mathcal{L}}
\newcommand{\empmod}{\mathcal{P}_{\text{emp}}}
\newcommand{\meta}{\phi}
\newcommand{\Meta}{\Phi}
\newcommand{\esl}{\Sigma}
\DeclareMathOperator{\expit}{expit}




% \title[About Beamer] %optional
\title{Super Learners}
\subtitle{and their oracle properties}
\author[Jinyang Liu (sqf320)]{Jinyang Liu (sqf320)}

\institute[UCPH] % (optional)
{
  Department of Mathematical Sciences\\
  University of Copenhagen
}

\date[VLC 2023]{June 2023}


%End of title page configuration block
%------------------------------------------------------------
%The next block of commands puts the table of contents at the 
%beginning of each section and highlights the current section:

\AtBeginSection[]
{
  \begin{frame}
    \frametitle{Overview}
    \tableofcontents[currentsection]
  \end{frame}
}

\setbeamertemplate{headline}{}
%------------------------------------------------------------
\begin{document}


\frame{\titlepage}
%---------------------------------------------------------
\section{Introduction}
\begin{frame}
    \frametitle{Introduction}
    \begin{block}{Binary regression}
        Let $ O = (Y, X) $ be an observation for $ Y \in \{0,1\} $ and $ X \in \mathcal{X}$ for $ \mathcal{X} \subseteq \mathbb{R}^{d} $. We assume that $ O \sim P $ for some $ P \in \mathcal{P} $.
    \end{block}
    Let $ \Theta = \{\btheta \mid \btheta : \mathcal{X} \to [0,1] \text{ measurable}\} $ be the set of \textbf{regression functions}. We would like to \textbf{estimate} a function $ \btheta \in \Theta $ such that the mean squared error (MSE) or risk
        $$
            R(\btheta , P) = \int L(O, \btheta) dP = \int (Y - \theta(X))^2 d P  
        $$
        is minimized. It turns out that the conditional expectation $$ x \mapsto E(Y \mid X = x) = P(Y \mid X = x) $$ is what minimizes the MSE. We refer to it as the \textbf{regression}.  
\end{frame}

\begin{frame}
    \frametitle{Example of a regression function}
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.85\textwidth]{figures/trueregression}
        \caption{Example of a pathological regression that can be difficult to learn using parametric techniques. Here a continuous outcome $ Y $ is plotted against a single continuous covariate $ X $ \citep{gyorfi2002distribution}.}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Linear approximation}
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.85\textwidth]{figures/linearestimate}
        \caption{Approximating the regression using linear regression, which is very biased \citep{gyorfi2002distribution}.}
    \end{figure}
\end{frame}

\subsection{Terminology}
\begin{frame}
    \frametitle{Terminology}
    We observe $ D_n = (O_1 , \dots , O_n) $, upon which we apply our \textbf{learning algorithms}  
    \begin{definition}[Learning algorithm]
        A learning algorithm is a measurable map $ \la : \mathcal{O}^{n} \to \Theta $ for $ n \in \mathbb{N} $. 
    \end{definition}
    We assume that the $ \la $ is well-defined for all $ n \in \mathbb{N} $ and that the ordering of the observations does not matter.  
    \begin{definition}[Learner or fitted learner]
        Let $ \la $ be a learning algorithm, a learner is the outcome of applying $ \la $ to our data $ D_n $ denoted as $ \la(D_n) $, which is a map in $ \Theta $.
    \end{definition}
    We usually have a \textbf{library of learning algorithms}, 
    \begin{align*}
        \lib = \{\la_{\q} \mid 1 \leq \q \leq\ml \},
    \end{align*}
    for which we can use to estimate the regression.  
\end{frame}

\subsection{Cross-validation}
\begin{frame}
\frametitle{$ K $-fold Cross-validation}
There is a one-to-one correspondance between our data $ D_n $ and the empirical measures over $ n $ observations
\begin{align*}
    P_n = \sum_{i = 1}^{n} \delta_{O_i},
\end{align*}
$ K $-fold cross-validation splits $ D_n $ into \textbf{validation} and \textbf{training} sets. The validation sets are indexed by $ s \in \{1, \dots , K\} $ and we denote the empirical measure over the validation set $ s $ as 
\begin{align*}
    P_{n,s}^{1} := \frac{1}{n_1} \sum_{i: s(i) = s} \delta_{O_i}, \qquad P_{n,s}^{0} := \frac{1}{n_0} \sum_{i: s(i) \neq s} \delta_{O_i}. 
\end{align*}
Here $ s(i) $ denotes whether $ O_i$ is in the validation set $ s $, and $ n_1, n_0 $ are the number of observations in the validation and training sets respectively. 
\end{frame}

\begin{frame}
\begin{exampleblock}{$ K $-fold cross-validation procedure}
Cross-validation is used to evaluate each algorithm, and is the central idea of the \textbf{super learner}:
    \begin{enumerate}
        \item Randomly split $ D_n $ into $ K $ disjoint and exhaustive validation sets
        \item For each $ s \in \{1, \dots , K\} $ fit each $ \la \in \lib $ on the training data $ P_{n, s}^{0} $ and obtain $ \la(P_{n, s}^{0}) $ 
        \item For each $ \la $, use $ \la(P_{n, s}^{0}) $ to predict on the validation set to obtain \textbf{level-1 covariates}: $$ Z_i = \left(\la_1(P_{n, s(i)}^{0})(X_i), \dots ,\la_\ml(P_{n, s(i)}^{0})(X_i)\right)  $$
        \item Calculate the MSE of $ \la $ on the validation set for $ s \in \{1, \dots, K \}$ 
            $$ R(\la(P_{n, s}^{0}) , P_{n, s}^{1}) = \frac{1}{n_1} \sum_{i : s(i) = s} \left(Y_i - \la(P_{n, s}^{0})(X_i)\right)^2 $$ 
    \end{enumerate}
\end{exampleblock}
\end{frame}

\section{Discrete Super Learner}
\begin{frame}
\frametitle{Discrete Super Learner}
Cross-validation allows us to select the algorithm with the lowest \textbf{empirical risk
}
\begin{align*}
    \hat{\la}_n := \argmin_{\la \in \lib} \frac{1}{K} \sum_{s = 1}^{K} R(\la(P_{n, s}^{0}) , P_{n, s}^{1}),  
\end{align*}
also known as the \textbf{cross-validation selected algorihm}. The \textbf{discrete super learner} is simply the cross-validation selected algorithm fitted on the entire dataset 
\begin{align*}
    X \mapsto \hat{\la}_n(P_n)(X).
\end{align*}
It is compared to the \textbf{oracle selected learning algorithm} that has the true minimum risk
\begin{align*}
    \tilde{\la}_n := \argmin_{\la \in \lib} \frac{1}{K} \sum_{s = 1}^{K} R(\la(P_{n, s}^{0}) , P).  
\end{align*}
\end{frame}

\subsection{Oracle Property}
\begin{frame}
    \frametitle{Discrete Super Learner: Asymptotic Equivalence}
    We let $ E_{D_n} $ denote the expectation wrt. the product measure of $ O_1 , \dots , O_n $.   
\begin{theorem}[Asymptotic equivalence] \label{cor:dslasymptoticequivalence}
    If there exists an $ \varepsilon > 0 $ such that 
   \begin{align*}
       E_{D_n} \frac{1}{K} \sum_{s = 1}^{K} R(\tilde{\la}_n(P_{n, s}^{0}), P) > \varepsilon \qquad \text{for all } n \in \mathbb{N},
   \end{align*}
   and if $ n_1 = f(n) $ for some polynomial function $ f $, then the risk of the super learner is asymptotically equivalent with the risk of the oracle selected learner, that is
   \begin{align*}
       \lim_{n \to \infty} \frac{E_{D_n} \frac{1}{K} \sum_{s = 1}^{K} R(\hat{\la}_n(P_{n, s}^{0}), P)}{E_{D_n} \frac{1}{K} \sum_{s = 1}^{K} R(\tilde{\la}_n(P_{n, s}^{0}), P)} = 1.
   \end{align*}
\end{theorem}

\end{frame}

\section{Ensemble Super Learner}
\begin{frame}
    \frametitle{Ensemble Super Learner} 
    Let $ \mathcal{Z} \subseteq [0,1]^{\ml} $ be the learners' out-of-fold predictions from doing $ K $-fold cross-validation (level 1 covariates).
    \begin{definition}[Level 1 data]
    The \textit{level 1 data}, $ \lone_{n} \subseteq \{0,1\} \times \mathcal{Z} $, is the observed $ Y_i $'s concatenated with the level 1 covariates:
    \begin{align*}
        \lone_{n} &= \{(Y_i ; Z_i)\}_{i = 1}^{n} \\
            &= \{(Y_i; \la_1(P_{n, s(i)}^{0})(X_i), \ldots, \la_\ml(P_{n, s(i)}^{0})(X_i)) \}_{i = 1}^n. 
    \end{align*}
    \end{definition}
    Let $ \mathcal{M} $ be the set of measurable functions, $ \meta: \mathcal{Z} \to [0,1] $, known as the set of \textbf{meta learners} 
    \begin{definition}[Meta learner]
        The \textit{meta learner} is a function $ \meta: \mathcal{Z} \to [0,1] $ in $ \mathcal{M} $ that maps the output of the candidate learners to a prediction. 
    \end{definition}
\end{frame}

\begin{frame}
    We estimate $ E(Y \mid Z) $ by applying a \textbf{meta learning algorithm} to the level 1 data. 
    \begin{definition}[Meta learning algorithm]
        A \textit{meta learning algorithm} $ \Meta $ is a measurable map that creates a meta learner from our level 1 data $ \lone_{n} \mapsto \Meta(\lone_{n}) \in \mathcal{M} $. 
    \end{definition}

\begin{definition}[Ensemble super learner]
    Let $ \meta = \Meta(\lone_{n}) $ be the outcome of applying a meta learning algorithm $ \Meta $ to the level 1 data, then the map 
    \begin{align*}
       x \mapsto \meta(\la_1(P_{n})(x), \ldots, \la_k(P_{n})(x) ),
    \end{align*}
    is called the \textit{ensemble super learner} and we will denote it by $ \esl(P_{n}) $. 
\end{definition}
Each learning algorithm is fitted on the entire dataset, and a meta learner is used to combine the predictions.
\end{frame}

\subsection{Constrained Regression}
\begin{frame}
    \frametitle{Ensemble Super Learner: Constrained Regression} 
    There are many choices of the meta learning algorithm, one can fit a weighted linear combination of the learning algorithms where 
    \begin{align*}
        \meta_{a}(z) = a \cdot z, \qquad \sum_{\q = 1}^{\ml} a_{\q} = 1, a_{\q} \geq 0 \text{ for all } \q,
    \end{align*}
    such that 
    \begin{align*}
        \esl(P_n)(x) &=  \meta_{a}(\la_{1}(P_n)(x) , \dots , \la_{\ml}(P_n)(x))\\
                     &= \sum_{\q = 1}^{\ml} a_{\q} \la_\q(P_n)(x) \in [0,1].
    \end{align*}
    The optimal weighting, $ a $, can be found by solving a \textbf{constrained least squares} on the level 1 data using quadratic programming techniques. 
\end{frame}

\section{Simulations}
\begin{frame}
    \frametitle{Simulations} 
    We consider a simulated dataset consisting of two covariates $ X_1, X_2 $ and a binary outcome $ Y \in \{0,1\} $ 
    \begin{align*}
        X_1 &\sim \operatorname{Unif}(0.5, 15),\\
        X_2 \mid X_1 = x_1 &\sim \mathcal{N}(3.5-0.03x_1, 1),\\
        Y \mid X_1 = x_1, X_2 = x_2 &\sim \operatorname{Ber}(\theta_0(x_1, x_2)),
    \end{align*}
    where $\theta_0(x_1, x_2) = \expit({-3.5 - 0.3x_1 + 0.85x_2 + 0.35x_1x_2})$. 
\end{frame}

\begin{frame}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/trueplot.pdf}
    \caption{The data-generating regression plotted as a heat map. The two covariates $ X_1 $ and $ X_2 $ are mapped by the regression function $ \theta_0 $ to a probability as indicated by the colors.}
    \label{fig:trueplot}
\end{figure}
\end{frame}


\begin{frame}
    \frametitle{Simulations: Library of Algorithms} 
The regression is captured by using logistic regression with interaction terms. We use the following library of learning algorithms:
\begin{enumerate}
    \item Intercept only logistic regression: $E[Y \mid X_1, X_2] = \expit(\beta_0)$
    \item Logistic regression with main effects: $E[Y \mid X_1, X_2] = \expit(\beta_0 + \beta_1 X_1 + \beta_2 X_2)$
    \item XGBoost with hyperparameters: \texttt{max\_depth=3, eta=0.3,\\ n\_rounds=100, objective='binary:logistic', booster='dart', nthread=5}
\end{enumerate}
The intercept model is included as a \textbf{baseline}, no learning algorithm should perform worse than the baseline. 
\end{frame}

\begin{frame}
    \frametitle{Simulations: Logistic Regression and XGBoost} 
    \begin{figure}
        \centering
        \centerline{\includegraphics[width=1.1\textwidth]{figures/predictpar.pdf}}
        \caption{The predictions of the main effects logistic regression and XGBoost fitted on $ 1,000 $ observations.}
        \label{fig:predictpar}
    \end{figure}
\end{frame}

\begin{frame}
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.8\textwidth]{figures/xgboost10k.pdf}
        \caption{XGBoost becoming better at approximating the regression as the sample size increases. Here the predictions of XGBoost are visualized for a training sample size of $ 10,000 $. }
        \label{fig:xgboost10k}
    \end{figure}
\end{frame}

\subsection{Validation risk and variance}
\begin{frame}
    \frametitle{Simulations: Risk and Variance over Training Samples} 
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.87\textwidth]{figures/losses_esl_s22.pdf}
        \caption{The validation risk of the super learners compared to other algorithms where the number of training samples are $n = 100, 200, \dots , N = 3,500 $.}
        \label{fig:losses_esl_s22}
    \end{figure}
\end{frame}

\begin{frame}
\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{figures/learner_vars_w_esl_1000.pdf}
    \caption{The variance of the super learners compared to other algorithms. Each algorithm is fitted $ K = 1,000 $ times on $n$ samples and used to predict $ K $ times on a single observation.}
    \label{fig:learner_vars_w_esl_1000}
\end{figure}
\end{frame}




\subsection{Locally weighted eSL}
\begin{frame}
    \frametitle{Locally Weighted Ensemble Super Learner} 
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.8\textwidth]{figures/esl_preds_xgboost_vs_main.pdf}
        \caption{Predictions of XGBoost vs main effects model. The learners' prediction on a single observation represent a point in the unit square, the point is colored by the probability obtained from applying the true regression function on that observation.}
        \label{fig:esl_preds_xgboost_vs_main}
    \end{figure}
\end{frame}

\begin{frame}
    \frametitle{Clustering level 1 covariates using $ k $-means} 
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.8\textwidth]{figures/esl_preds_xgboost_vs_main_kmeans.pdf}
    \caption{The predictions are clustered into 4 groups using $ k $-means clustering.}
        \label{fig:esl_preds_xgboost_vs_main_kmeans}
    \end{figure}
\end{frame}


\end{document}
